{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tensor Shape: torch.Size([1278, 39])\n",
      "Test Tensor Shape: torch.Size([320, 39])\n",
      "Device Used: cpu\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('../data/physionet_wo_missing.csv', index_col=0)\n",
    "data = df.to_numpy()\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32).to(device)\n",
    "test_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Verify Shapes\n",
    "print(f\"Train Tensor Shape: {train_tensor.shape}\")\n",
    "print(f\"Test Tensor Shape: {test_tensor.shape}\")\n",
    "print(f\"Device Used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1278, 39])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, d_model=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(1, d_model)  # Embed each feature (column) into d_model dimensions\n",
    "\n",
    "        self.column_embedding = nn.Embedding(num_features, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=128, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, 1)  # Predict one value per feature\n",
    "\n",
    "    def forward(self, x, column_indices, mask=None):\n",
    "\n",
    "        x = x.unsqueeze(-1)\n",
    "\n",
    "        x_embed = self.embedding(x)\n",
    "\n",
    "        column_embed = self.column_embedding(column_indices)\n",
    "        x_embed += column_embed.unsqueeze(0)\n",
    "\n",
    "        x_encoded = self.transformer_encoder(x_embed, mask=mask)\n",
    "\n",
    "        output = self.output_layer(x_encoded)\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "num_features = train_tensor.shape[1]\n",
    "model = TransformerModel(num_features=num_features, d_model=64, num_heads=4, num_layers=2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "column_indices = torch.arange(num_features).to(device)\n",
    "\n",
    "output = model(train_tensor, column_indices)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Shape: torch.Size([1278, 39])\n"
     ]
    }
   ],
   "source": [
    "def create_missing_mask(data, missing_fraction=0.2):\n",
    "    mask = torch.rand(data.shape).to(data.device) < missing_fraction\n",
    "    return mask.int()\n",
    "\n",
    "missing_fraction = 0.2\n",
    "mask = create_missing_mask(train_tensor, missing_fraction)\n",
    "print(f\"Mask Shape: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, ground_truth, mask):\n",
    "    mse_loss = nn.MSELoss(reduction='none')\n",
    "    loss = mse_loss(predictions, ground_truth)\n",
    "    masked_loss = (loss * mask).sum() / mask.sum()  # Normalize by number of masked positions\n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.3988\n",
      "Epoch 2/20, Loss: 1.3220\n",
      "Epoch 3/20, Loss: 1.2824\n",
      "Epoch 4/20, Loss: 1.0305\n",
      "Epoch 5/20, Loss: 1.0728\n",
      "Epoch 6/20, Loss: 1.2077\n",
      "Epoch 7/20, Loss: 1.1204\n",
      "Epoch 8/20, Loss: 0.9913\n",
      "Epoch 9/20, Loss: 1.0744\n",
      "Epoch 10/20, Loss: 1.0456\n",
      "Epoch 11/20, Loss: 1.0623\n",
      "Epoch 12/20, Loss: 0.9554\n",
      "Epoch 13/20, Loss: 1.1202\n",
      "Epoch 14/20, Loss: 0.9898\n",
      "Epoch 15/20, Loss: 0.9885\n",
      "Epoch 16/20, Loss: 1.0381\n",
      "Epoch 17/20, Loss: 1.0451\n",
      "Epoch 18/20, Loss: 1.0418\n",
      "Epoch 19/20, Loss: 1.0344\n",
      "Epoch 20/20, Loss: 0.9904\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    mask = create_missing_mask(train_tensor, missing_fraction)\n",
    "    input_with_mask = train_tensor.clone()\n",
    "    input_with_mask[mask == 1] = 0\n",
    "\n",
    "    predictions = model(input_with_mask, column_indices)\n",
    "\n",
    "    loss = compute_loss(predictions, train_tensor, mask)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.3810)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_data, column_indices, missing_fraction=0.2):\n",
    "    model.eval()\n",
    "\n",
    "    mask = create_missing_mask(test_data, missing_fraction)\n",
    "    input_with_mask = test_data.clone()\n",
    "    input_with_mask[mask == 1] = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "\n",
    "    loss = compute_loss(predictions, test_data, mask)\n",
    "    print(f\"Test Loss: {loss.item():.4f}\")\n",
    "    return loss\n",
    "\n",
    "evaluate_model(model, test_tensor, column_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE: 0.0253\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()\n",
    "\n",
    "def evaluate_model_with_nrmse(model, test_data, column_indices, missing_fraction=0.2):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    mask = create_missing_mask(test_data, missing_fraction)\n",
    "    input_with_mask = test_data.clone()\n",
    "    input_with_mask[mask == 1] = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "\n",
    "    nrmse = compute_nrmse(predictions, test_data, mask)\n",
    "    print(f\"NRMSE: {nrmse:.4f}\")\n",
    "    return nrmse\n",
    "\n",
    "nrmse = evaluate_model_with_nrmse(model, test_tensor, column_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyampute.ampute import MultivariateAmputation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def create_missing_dataset(data, missing_fraction=0.1, mechanism=\"MCAR\"):\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "    patterns = [{\n",
    "        \"incomplete_vars\": data.columns.tolist(),\n",
    "        \"weights\": np.zeros(len(data.columns)),  # Default for MCAR\n",
    "        \"mechanism\": mechanism,\n",
    "        \"score_to_probability_func\": \"sigmoid-right\"\n",
    "    }]\n",
    "\n",
    "    if mechanism == \"MAR\":\n",
    "        num_columns = len(data.columns)\n",
    "        num_amputed_columns = max(1, int(num_columns * 0.5))\n",
    "        amputed_columns = np.random.choice(data.columns, num_amputed_columns, replace=False)\n",
    "        patterns[0][\"incomplete_vars\"] = amputed_columns\n",
    "        patterns[0][\"weights\"] = np.random.uniform(-1, 1, num_columns) \n",
    "    \n",
    "    elif mechanism == \"MNAR\":\n",
    "        patterns[0][\"weights\"] = np.random.uniform(0.5, 2, len(data.columns))\n",
    "\n",
    "    amputer = MultivariateAmputation(prop=missing_fraction, patterns=patterns)\n",
    "    amputed_data = amputer.fit_transform(data)\n",
    "\n",
    "    mask = pd.isna(amputed_data).astype(int).to_numpy()\n",
    "    amputed_data = np.nan_to_num(amputed_data, nan=0.0)\n",
    "\n",
    "    amputed_data = torch.tensor(amputed_data, dtype=torch.float32).to(device)\n",
    "    mask = torch.tensor(mask, dtype=torch.int32).to(device)\n",
    "\n",
    "    assert amputed_data.shape == mask.shape, \"Data and mask shapes dont match\"\n",
    "    return amputed_data, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_missingness(model, original_data, column_indices, missing_fraction=0.1, mechanism=\"MCAR\"):\n",
    "    model.eval()\n",
    "\n",
    "    amputed_data, mask = create_missing_dataset(original_data.cpu().numpy(), missing_fraction, mechanism)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(amputed_data, column_indices)\n",
    "\n",
    "    nrmse = compute_nrmse(predictions, original_data, mask)\n",
    "    print(f\"{mechanism} NRMSE at {missing_fraction * 100:.0f}% Missing: {nrmse:.4f}\")\n",
    "    return nrmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_across_mechanisms(model, original_data, column_indices, missing_fractions):\n",
    "    mechanisms = [\"MCAR\", \"MAR\", \"MNAR\"]\n",
    "    results = {mechanism: {} for mechanism in mechanisms}\n",
    "    \n",
    "    for mechanism in mechanisms:\n",
    "        for fraction in missing_fractions:\n",
    "            results[mechanism][fraction] = evaluate_model_with_missingness(\n",
    "                model, original_data, column_indices, missing_fraction=fraction, mechanism=mechanism\n",
    "            )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 04:03:19,086 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,120 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,143 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,164 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,185 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,205 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,206 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:19,207 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-28 04:03:19,234 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,235 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:19,236 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-28 04:03:19,262 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,263 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:19,264 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-28 04:03:19,289 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,290 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:19,292 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-28 04:03:19,318 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,319 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAR NRMSE at 10% Missing: 0.0233\n",
      "MCAR NRMSE at 20% Missing: 0.0282\n",
      "MCAR NRMSE at 30% Missing: 0.0301\n",
      "MCAR NRMSE at 40% Missing: 0.0293\n",
      "MCAR NRMSE at 50% Missing: 0.0298\n",
      "MAR NRMSE at 10% Missing: 0.0323\n",
      "MAR NRMSE at 20% Missing: 0.0309\n",
      "MAR NRMSE at 30% Missing: 0.0251\n",
      "MAR NRMSE at 40% Missing: 0.0246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 04:03:19,320 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-28 04:03:19,349 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,351 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-28 04:03:19,376 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,378 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-28 04:03:19,423 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,424 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-28 04:03:19,450 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,451 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-28 04:03:19,476 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:19,478 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAR NRMSE at 50% Missing: 0.0285\n",
      "MNAR NRMSE at 10% Missing: 0.0355\n",
      "MNAR NRMSE at 20% Missing: 0.0352\n",
      "MNAR NRMSE at 30% Missing: 0.0319\n",
      "MNAR NRMSE at 40% Missing: 0.0262\n",
      "MNAR NRMSE at 50% Missing: 0.0292\n",
      "\n",
      "MCAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0233\n",
      "  Missing Fraction 20%: NRMSE = 0.0282\n",
      "  Missing Fraction 30%: NRMSE = 0.0301\n",
      "  Missing Fraction 40%: NRMSE = 0.0293\n",
      "  Missing Fraction 50%: NRMSE = 0.0298\n",
      "\n",
      "MAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0323\n",
      "  Missing Fraction 20%: NRMSE = 0.0309\n",
      "  Missing Fraction 30%: NRMSE = 0.0251\n",
      "  Missing Fraction 40%: NRMSE = 0.0246\n",
      "  Missing Fraction 50%: NRMSE = 0.0285\n",
      "\n",
      "MNAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0355\n",
      "  Missing Fraction 20%: NRMSE = 0.0352\n",
      "  Missing Fraction 30%: NRMSE = 0.0319\n",
      "  Missing Fraction 40%: NRMSE = 0.0262\n",
      "  Missing Fraction 50%: NRMSE = 0.0292\n"
     ]
    }
   ],
   "source": [
    "missing_fractions = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "results = evaluate_across_mechanisms(model, test_tensor, column_indices, missing_fractions)\n",
    "\n",
    "for mechanism, nrmse_values in results.items():\n",
    "    print(f\"\\n{mechanism} Results:\")\n",
    "    for frac, nrmse in nrmse_values.items():\n",
    "        print(f\"  Missing Fraction {frac * 100:.0f}%: NRMSE = {nrmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to tabular_transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"tabular_transformer_model.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 04:03:24,105 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,118 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,127 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,137 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,148 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,157 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,158 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:24,159 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,168 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,176 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,177 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:24,178 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,186 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,194 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,195 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:24,196 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,205 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,212 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,213 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:24,215 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,223 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,231 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,232 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:24,233 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,241 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,249 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,250 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,258 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,265 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,267 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,273 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,281 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,282 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,289 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,296 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,298 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,306 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,313 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,315 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAR NRMSE at 10% Missing: 0.0508\n",
      "MCAR NRMSE at 20% Missing: 0.0505\n",
      "MCAR NRMSE at 30% Missing: 0.0482\n",
      "MCAR NRMSE at 40% Missing: 0.0477\n",
      "MCAR NRMSE at 50% Missing: 0.0478\n",
      "MAR NRMSE at 10% Missing: 0.0431\n",
      "MAR NRMSE at 20% Missing: 0.0514\n",
      "MAR NRMSE at 30% Missing: 0.0496\n",
      "MAR NRMSE at 40% Missing: 0.0414\n",
      "MAR NRMSE at 50% Missing: 0.0590\n",
      "MNAR NRMSE at 10% Missing: 0.0603\n",
      "MNAR NRMSE at 20% Missing: 0.0521\n",
      "MNAR NRMSE at 30% Missing: 0.0480\n",
      "MNAR NRMSE at 40% Missing: 0.0466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,322 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNAR NRMSE at 50% Missing: 0.0470\n",
      "\n",
      "MCAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0508\n",
      "  Missing Fraction 20%: NRMSE = 0.0505\n",
      "  Missing Fraction 30%: NRMSE = 0.0482\n",
      "  Missing Fraction 40%: NRMSE = 0.0477\n",
      "  Missing Fraction 50%: NRMSE = 0.0478\n",
      "\n",
      "MAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0431\n",
      "  Missing Fraction 20%: NRMSE = 0.0514\n",
      "  Missing Fraction 30%: NRMSE = 0.0496\n",
      "  Missing Fraction 40%: NRMSE = 0.0414\n",
      "  Missing Fraction 50%: NRMSE = 0.0590\n",
      "\n",
      "MNAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0603\n",
      "  Missing Fraction 20%: NRMSE = 0.0521\n",
      "  Missing Fraction 30%: NRMSE = 0.0480\n",
      "  Missing Fraction 40%: NRMSE = 0.0466\n",
      "  Missing Fraction 50%: NRMSE = 0.0470\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.read_csv('synthetic_physionet_data.csv', index_col=0)\n",
    "new_data = new_df.to_numpy()\n",
    "\n",
    "new_test_data = scaler.transform(new_data)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "new_test_tensor = torch.tensor(new_test_data, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "new_results = evaluate_across_mechanisms(model, new_test_tensor, column_indices, missing_fractions)\n",
    "\n",
    "for mechanism, nrmse_values in new_results.items():\n",
    "    print(f\"\\n{mechanism} Results:\")\n",
    "    for frac, nrmse in nrmse_values.items():\n",
    "        print(f\"  Missing Fraction {frac * 100:.0f}%: NRMSE = {nrmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
