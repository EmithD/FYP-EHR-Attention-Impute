{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tensor Shape: torch.Size([1278, 39])\n",
      "Test Tensor Shape: torch.Size([320, 39])\n",
      "Device Used: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Load Dataset\n",
    "# Replace 'dataset.csv' with your dataset file path\n",
    "df = pd.read_csv('../data/physionet_wo_missing.csv', index_col=0)\n",
    "\n",
    "# Convert dataset to numpy array (assuming all features are numeric)\n",
    "data = df.to_numpy()\n",
    "\n",
    "# Split Dataset\n",
    "# Specify the test size (e.g., 20% for testing)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize Data\n",
    "# Use StandardScaler to normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "# Convert to Torch Tensors\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32).to(device)\n",
    "test_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Verify Shapes\n",
    "print(f\"Train Tensor Shape: {train_tensor.shape}\")\n",
    "print(f\"Test Tensor Shape: {test_tensor.shape}\")\n",
    "print(f\"Device Used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1278, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emith\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_features, d_model=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(TabularTransformer, self).__init__()\n",
    "        \n",
    "        # 1) Column Embedding Layer\n",
    "        self.embedding = nn.Linear(1, d_model)  # Embed each feature (column) into d_model dimensions\n",
    "        \n",
    "        # 2) Positional Encoding (Column Embedding)\n",
    "        self.column_embedding = nn.Embedding(num_features, d_model)\n",
    "        \n",
    "        # 3) Transformer Encoder Layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=128, \n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Use batch-first format for better performance\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # 4) Output Layer\n",
    "        self.output_layer = nn.Linear(d_model, 1)  # Predict one value per feature\n",
    "\n",
    "    def forward(self, x, column_indices, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, num_features]\n",
    "        column_indices: Tensor of column indices for embedding (shape: [num_features])\n",
    "        mask: Optional mask for self-attention (shape: [batch_size, num_features, num_features])\n",
    "        \"\"\"\n",
    "        # Reshape x to [batch_size, num_features, 1] for embedding\n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        # Column Embedding\n",
    "        x_embed = self.embedding(x)\n",
    "        \n",
    "        # Add Positional (Column) Embedding\n",
    "        column_embed = self.column_embedding(column_indices)\n",
    "        x_embed += column_embed.unsqueeze(0)  # Broadcast across batch\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        x_encoded = self.transformer_encoder(x_embed, mask=mask)\n",
    "        \n",
    "        # Final Output Layer (Predict missing values)\n",
    "        output = self.output_layer(x_encoded)\n",
    "        return output.squeeze(-1)  # [batch_size, num_features]\n",
    "\n",
    "# Model Initialization\n",
    "num_features = train_tensor.shape[1]  # Number of features (columns in dataset)\n",
    "model = TabularTransformer(num_features=num_features, d_model=64, num_heads=4, num_layers=2)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Column Indices\n",
    "column_indices = torch.arange(num_features).to(device)\n",
    "\n",
    "# Example Forward Pass\n",
    "output = model(train_tensor, column_indices)\n",
    "print(f\"Output shape: {output.shape}\")  # Should match [batch_size, num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Shape: torch.Size([1278, 39])\n"
     ]
    }
   ],
   "source": [
    "def create_missing_mask(data, missing_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Creates a binary mask for missing values.\n",
    "\n",
    "    Args:\n",
    "        data (Tensor): Input data (shape: [batch_size, num_features]).\n",
    "        missing_fraction (float): Fraction of values to mask.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Binary mask of the same shape as `data` (1 = missing, 0 = observed).\n",
    "    \"\"\"\n",
    "    mask = torch.rand(data.shape).to(data.device) < missing_fraction\n",
    "    return mask.int()\n",
    "\n",
    "# Example usage\n",
    "missing_fraction = 0.2  # Mask 20% of the data\n",
    "mask = create_missing_mask(train_tensor, missing_fraction)\n",
    "print(f\"Mask Shape: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Computes the loss only on masked positions.\n",
    "\n",
    "    Args:\n",
    "        predictions (Tensor): Model predictions (shape: [batch_size, num_features]).\n",
    "        ground_truth (Tensor): Original data (shape: [batch_size, num_features]).\n",
    "        mask (Tensor): Binary mask (1 = missing, 0 = observed).\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Scalar loss value.\n",
    "    \"\"\"\n",
    "    mse_loss = nn.MSELoss(reduction='none')\n",
    "    loss = mse_loss(predictions, ground_truth)\n",
    "    masked_loss = (loss * mask).sum() / mask.sum()  # Normalize by number of masked positions\n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.2442\n",
      "Epoch 2/10, Loss: 1.3574\n",
      "Epoch 3/10, Loss: 1.1146\n",
      "Epoch 4/10, Loss: 1.0673\n",
      "Epoch 5/10, Loss: 1.0662\n",
      "Epoch 6/10, Loss: 1.0245\n",
      "Epoch 7/10, Loss: 1.0768\n",
      "Epoch 8/10, Loss: 0.9893\n",
      "Epoch 9/10, Loss: 0.9766\n",
      "Epoch 10/10, Loss: 1.0551\n"
     ]
    }
   ],
   "source": [
    "# Define Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Generate masks and replace masked values with 0\n",
    "    mask = create_missing_mask(train_tensor, missing_fraction)\n",
    "    input_with_mask = train_tensor.clone()\n",
    "    input_with_mask[mask == 1] = 0  # Replace masked positions with 0\n",
    "    \n",
    "    # Forward Pass\n",
    "    predictions = model(input_with_mask, column_indices)\n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = compute_loss(predictions, train_tensor, mask)\n",
    "    \n",
    "    # Backward Pass and Optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.1588, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_data, column_indices, missing_fraction=0.2):\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate masks and replace masked values with 0\n",
    "    mask = create_missing_mask(test_data, missing_fraction)\n",
    "    input_with_mask = test_data.clone()\n",
    "    input_with_mask[mask == 1] = 0  # Replace masked positions with 0\n",
    "    \n",
    "    # Forward Pass\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = compute_loss(predictions, test_data, mask)\n",
    "    print(f\"Test Loss: {loss.item():.4f}\")\n",
    "    return loss\n",
    "\n",
    "# Evaluate the Model\n",
    "evaluate_model(model, test_tensor, column_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE: 0.0254\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Computes the Normalized Root Mean Squared Error (NRMSE) for imputed values.\n",
    "\n",
    "    Args:\n",
    "        predictions (Tensor): Model predictions (shape: [batch_size, num_features]).\n",
    "        ground_truth (Tensor): Original data (shape: [batch_size, num_features]).\n",
    "        mask (Tensor): Binary mask (1 = missing, 0 = observed).\n",
    "\n",
    "    Returns:\n",
    "        float: NRMSE value for the masked positions.\n",
    "    \"\"\"\n",
    "    # Extract only masked values\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "    \n",
    "    # Compute RMSE\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    # Compute range of ground truth values\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "    \n",
    "    # Normalize RMSE\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()\n",
    "\n",
    "def evaluate_model_with_nrmse(model, test_data, column_indices, missing_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Evaluates the model and computes NRMSE for masked positions.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        test_data (Tensor): Test dataset (shape: [batch_size, num_features]).\n",
    "        column_indices (Tensor): Column indices for embedding (shape: [num_features]).\n",
    "        missing_fraction (float): Fraction of values to mask.\n",
    "\n",
    "    Returns:\n",
    "        float: NRMSE value for the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate masks and replace masked values with 0\n",
    "    mask = create_missing_mask(test_data, missing_fraction)\n",
    "    input_with_mask = test_data.clone()\n",
    "    input_with_mask[mask == 1] = 0  # Replace masked positions with 0\n",
    "    \n",
    "    # Forward Pass\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "    \n",
    "    # Compute NRMSE\n",
    "    nrmse = compute_nrmse(predictions, test_data, mask)\n",
    "    print(f\"NRMSE: {nrmse:.4f}\")\n",
    "    return nrmse\n",
    "\n",
    "# Evaluate the Model with NRMSE\n",
    "nrmse = evaluate_model_with_nrmse(model, test_tensor, column_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyampute.ampute import MultivariateAmputation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Step 1: Create MCAR Dataset with pyampute\n",
    "def create_mcar_dataset(data, missing_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Introduces MCAR missingness into the dataset and ensures mask matches the data.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray or pd.DataFrame): Original dataset (numerical).\n",
    "        missing_fraction (float): Fraction of missing values to introduce.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Data with missing values replaced by 0.0.\n",
    "        Tensor: Binary mask where 1 = missing, 0 = observed.\n",
    "    \"\"\"\n",
    "    # Convert data to DataFrame for pyampute\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = pd.DataFrame(data)\n",
    "    \n",
    "    # Apply MCAR missingness using pyampute\n",
    "    amputer = MultivariateAmputation(prop=missing_fraction)\n",
    "    amputed_data = amputer.fit_transform(data)\n",
    "    \n",
    "    # Create mask: 1 for missing values, 0 for observed\n",
    "    mask = pd.isna(amputed_data).astype(int).to_numpy()  # Ensure correct shape\n",
    "    amputed_data = np.nan_to_num(amputed_data, nan=0.0)  # Replace NaN with 0.0\n",
    "    \n",
    "    # Convert back to torch tensors\n",
    "    amputed_data = torch.tensor(amputed_data, dtype=torch.float32).to(device)\n",
    "    mask = torch.tensor(mask, dtype=torch.int32).to(device)\n",
    "    \n",
    "    # Verify shapes\n",
    "    assert amputed_data.shape == mask.shape, \"Data and mask shapes do not match!\"\n",
    "    \n",
    "    return amputed_data, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute NRMSE\n",
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Computes the Normalized Root Mean Squared Error (NRMSE) for imputed values.\n",
    "\n",
    "    Args:\n",
    "        predictions (Tensor): Model predictions (shape: [batch_size, num_features]).\n",
    "        ground_truth (Tensor): Original data (shape: [batch_size, num_features]).\n",
    "        mask (Tensor): Binary mask (1 = missing, 0 = observed).\n",
    "\n",
    "    Returns:\n",
    "        float: NRMSE value for the masked positions.\n",
    "    \"\"\"\n",
    "    # Extract only masked values\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "    \n",
    "    # Compute RMSE\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    # Compute range of ground truth values\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "    \n",
    "    # Normalize RMSE\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Evaluate Model\n",
    "def evaluate_model_with_mcar(model, original_data, column_indices, missing_fraction=0.1):\n",
    "    \"\"\"\n",
    "    Evaluates the model using an MCAR dataset and computes NRMSE.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        original_data (Tensor): Original dataset (shape: [batch_size, num_features]).\n",
    "        column_indices (Tensor): Column indices for embedding (shape: [num_features]).\n",
    "        missing_fraction (float): Fraction of missing values to introduce.\n",
    "\n",
    "    Returns:\n",
    "        float: NRMSE value for the MCAR dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate MCAR dataset\n",
    "    amputed_data, mask = create_mcar_dataset(original_data.cpu().numpy(), missing_fraction)\n",
    "    \n",
    "    # Forward pass with amputed data\n",
    "    with torch.no_grad():\n",
    "        predictions = model(amputed_data, column_indices)\n",
    "    \n",
    "    # Compute NRMSE\n",
    "    nrmse = compute_nrmse(predictions, original_data, mask)\n",
    "    print(f\"NRMSE on MCAR Data: {nrmse:.4f}\")\n",
    "    return nrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 03:51:19,317 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-27 03:51:19,320 [WARNING] Binary variables (at indices [ 3  4  7 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE on MCAR Data: 0.0284\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Evaluate\n",
    "nrmse_mcar = evaluate_model_with_mcar(model, test_tensor, column_indices, missing_fraction=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to tabular_transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Path to save the model\n",
    "model_path = \"tabular_transformer_model.pth\"\n",
    "\n",
    "# Save the model's state dictionary (recommended)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
