{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tensor Shape: torch.Size([1278, 39])\n",
      "Test Tensor Shape: torch.Size([320, 39])\n",
      "Device Used: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('../data/physionet_wo_missing.csv', index_col=0)\n",
    "data = df.to_numpy()\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32).to(device)\n",
    "test_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Verify Shapes\n",
    "print(f\"Train Tensor Shape: {train_tensor.shape}\")\n",
    "print(f\"Test Tensor Shape: {test_tensor.shape}\")\n",
    "print(f\"Device Used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1278, 39])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, d_model=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(1, d_model)  # Embed each feature (column) into d_model dimensions\n",
    "\n",
    "        self.column_embedding = nn.Embedding(num_features, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=128, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, 1)  # Predict one value per feature\n",
    "\n",
    "    def forward(self, x, column_indices, mask=None):\n",
    "\n",
    "        x = x.unsqueeze(-1)\n",
    "\n",
    "        x_embed = self.embedding(x)\n",
    "\n",
    "        column_embed = self.column_embedding(column_indices)\n",
    "        x_embed += column_embed.unsqueeze(0)\n",
    "\n",
    "        x_encoded = self.transformer_encoder(x_embed, mask=mask)\n",
    "\n",
    "        output = self.output_layer(x_encoded)\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "num_features = train_tensor.shape[1]\n",
    "model = TransformerModel(num_features=num_features, d_model=64, num_heads=4, num_layers=2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "column_indices = torch.arange(num_features).to(device)\n",
    "\n",
    "output = model(train_tensor, column_indices)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Shape: torch.Size([1278, 39])\n"
     ]
    }
   ],
   "source": [
    "def create_missing_mask(data, missing_fraction=0.2):\n",
    "    mask = torch.rand(data.shape).to(data.device) < missing_fraction\n",
    "    return mask.int()\n",
    "\n",
    "missing_fraction = 0.2\n",
    "mask = create_missing_mask(train_tensor, missing_fraction)\n",
    "print(f\"Mask Shape: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, ground_truth, mask):\n",
    "    mse_loss = nn.MSELoss(reduction='none')\n",
    "    loss = mse_loss(predictions, ground_truth)\n",
    "    masked_loss = (loss * mask).sum() / mask.sum()  # Normalize by number of masked positions\n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.2792\n",
      "Epoch 2/20, Loss: 1.4445\n",
      "Epoch 3/20, Loss: 1.0932\n",
      "Epoch 4/20, Loss: 1.1622\n",
      "Epoch 5/20, Loss: 1.0769\n",
      "Epoch 6/20, Loss: 1.0852\n",
      "Epoch 7/20, Loss: 1.0443\n",
      "Epoch 8/20, Loss: 1.0396\n",
      "Epoch 9/20, Loss: 1.0411\n",
      "Epoch 10/20, Loss: 1.0314\n",
      "Epoch 11/20, Loss: 0.9263\n",
      "Epoch 12/20, Loss: 0.9662\n",
      "Epoch 13/20, Loss: 1.0414\n",
      "Epoch 14/20, Loss: 0.9926\n",
      "Epoch 15/20, Loss: 0.9712\n",
      "Epoch 16/20, Loss: 1.0472\n",
      "Epoch 17/20, Loss: 0.9842\n",
      "Epoch 18/20, Loss: 1.0551\n",
      "Epoch 19/20, Loss: 1.0399\n",
      "Epoch 20/20, Loss: 0.9754\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    mask = create_missing_mask(train_tensor, missing_fraction)\n",
    "    input_with_mask = train_tensor.clone()\n",
    "    input_with_mask[mask == 1] = 0\n",
    "\n",
    "    predictions = model(input_with_mask, column_indices)\n",
    "\n",
    "    loss = compute_loss(predictions, train_tensor, mask)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1057\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.1057, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_data, column_indices, missing_fraction=0.2):\n",
    "    model.eval()\n",
    "\n",
    "    mask = create_missing_mask(test_data, missing_fraction)\n",
    "    input_with_mask = test_data.clone()\n",
    "    input_with_mask[mask == 1] = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "\n",
    "    loss = compute_loss(predictions, test_data, mask)\n",
    "    print(f\"Test Loss: {loss.item():.4f}\")\n",
    "    return loss\n",
    "\n",
    "evaluate_model(model, test_tensor, column_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE: 0.0279\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()\n",
    "\n",
    "def evaluate_model_with_nrmse(model, test_data, column_indices, missing_fraction=0.2):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    mask = create_missing_mask(test_data, missing_fraction)\n",
    "    input_with_mask = test_data.clone()\n",
    "    input_with_mask[mask == 1] = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "\n",
    "    nrmse = compute_nrmse(predictions, test_data, mask)\n",
    "    print(f\"NRMSE: {nrmse:.4f}\")\n",
    "    return nrmse\n",
    "\n",
    "nrmse = evaluate_model_with_nrmse(model, test_tensor, column_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyampute.ampute import MultivariateAmputation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def create_missing_dataset(data, missing_fraction=0.1, mechanism=\"MCAR\"):\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "    patterns = [{\n",
    "        \"incomplete_vars\": data.columns.tolist(),\n",
    "        \"weights\": np.zeros(len(data.columns)),  # Default for MCAR\n",
    "        \"mechanism\": mechanism,\n",
    "        \"score_to_probability_func\": \"sigmoid-right\"\n",
    "    }]\n",
    "\n",
    "    if mechanism == \"MAR\":\n",
    "        num_columns = len(data.columns)\n",
    "        num_amputed_columns = max(1, int(num_columns * 0.5))\n",
    "        amputed_columns = np.random.choice(data.columns, num_amputed_columns, replace=False)\n",
    "        patterns[0][\"incomplete_vars\"] = amputed_columns\n",
    "        patterns[0][\"weights\"] = np.random.uniform(-1, 1, num_columns) \n",
    "    \n",
    "    elif mechanism == \"MNAR\":\n",
    "        patterns[0][\"weights\"] = np.random.uniform(0.5, 2, len(data.columns))\n",
    "\n",
    "    amputer = MultivariateAmputation(prop=missing_fraction, patterns=patterns)\n",
    "    amputed_data = amputer.fit_transform(data)\n",
    "\n",
    "    mask = pd.isna(amputed_data).astype(int).to_numpy()\n",
    "    amputed_data = np.nan_to_num(amputed_data, nan=0.0)\n",
    "\n",
    "    amputed_data = torch.tensor(amputed_data, dtype=torch.float32).to(device)\n",
    "    mask = torch.tensor(mask, dtype=torch.int32).to(device)\n",
    "\n",
    "    assert amputed_data.shape == mask.shape, \"Data and mask shapes dont match\"\n",
    "    return amputed_data, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_missingness(model, original_data, column_indices, missing_fraction=0.1, mechanism=\"MCAR\"):\n",
    "    model.eval()\n",
    "\n",
    "    amputed_data, mask = create_missing_dataset(original_data.cpu().numpy(), missing_fraction, mechanism)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(amputed_data, column_indices)\n",
    "\n",
    "    nrmse = compute_nrmse(predictions, original_data, mask)\n",
    "    print(f\"{mechanism} NRMSE at {missing_fraction * 100:.0f}% Missing: {nrmse:.4f}\")\n",
    "    return nrmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_across_mechanisms(model, original_data, column_indices, missing_fractions):\n",
    "    mechanisms = [\"MCAR\", \"MAR\", \"MNAR\"]\n",
    "    results = {mechanism: {} for mechanism in mechanisms}\n",
    "    \n",
    "    for mechanism in mechanisms:\n",
    "        for fraction in missing_fractions:\n",
    "            results[mechanism][fraction] = evaluate_model_with_missingness(\n",
    "                model, original_data, column_indices, missing_fraction=fraction, mechanism=mechanism\n",
    "            )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 14:31:43,341 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,352 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,363 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,374 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,384 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,398 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,399 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-01 14:31:43,401 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-01 14:31:43,426 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,427 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-01 14:31:43,428 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-01 14:31:43,451 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,452 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-01 14:31:43,455 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-01 14:31:43,475 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,476 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-01 14:31:43,478 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-01 14:31:43,497 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,498 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-01 14:31:43,500 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-01 14:31:43,519 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,521 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-01 14:31:43,540 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAR NRMSE at 10% Missing: 0.0342\n",
      "MCAR NRMSE at 20% Missing: 0.0284\n",
      "MCAR NRMSE at 30% Missing: 0.0269\n",
      "MCAR NRMSE at 40% Missing: 0.0243\n",
      "MCAR NRMSE at 50% Missing: 0.0261\n",
      "MAR NRMSE at 10% Missing: 0.0212\n",
      "MAR NRMSE at 20% Missing: 0.0334\n",
      "MAR NRMSE at 30% Missing: 0.0338\n",
      "MAR NRMSE at 40% Missing: 0.0251\n",
      "MAR NRMSE at 50% Missing: 0.0278\n",
      "MNAR NRMSE at 10% Missing: 0.0407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-01 14:31:43,542 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-01 14:31:43,564 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,566 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-01 14:31:43,588 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,590 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-02-01 14:31:43,610 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-01 14:31:43,612 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNAR NRMSE at 20% Missing: 0.0334\n",
      "MNAR NRMSE at 30% Missing: 0.0311\n",
      "MNAR NRMSE at 40% Missing: 0.0302\n",
      "MNAR NRMSE at 50% Missing: 0.0305\n",
      "\n",
      "MCAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0342\n",
      "  Missing Fraction 20%: NRMSE = 0.0284\n",
      "  Missing Fraction 30%: NRMSE = 0.0269\n",
      "  Missing Fraction 40%: NRMSE = 0.0243\n",
      "  Missing Fraction 50%: NRMSE = 0.0261\n",
      "\n",
      "MAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0212\n",
      "  Missing Fraction 20%: NRMSE = 0.0334\n",
      "  Missing Fraction 30%: NRMSE = 0.0338\n",
      "  Missing Fraction 40%: NRMSE = 0.0251\n",
      "  Missing Fraction 50%: NRMSE = 0.0278\n",
      "\n",
      "MNAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0407\n",
      "  Missing Fraction 20%: NRMSE = 0.0334\n",
      "  Missing Fraction 30%: NRMSE = 0.0311\n",
      "  Missing Fraction 40%: NRMSE = 0.0302\n",
      "  Missing Fraction 50%: NRMSE = 0.0305\n"
     ]
    }
   ],
   "source": [
    "missing_fractions = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "results = evaluate_across_mechanisms(model, test_tensor, column_indices, missing_fractions)\n",
    "\n",
    "for mechanism, nrmse_values in results.items():\n",
    "    print(f\"\\n{mechanism} Results:\")\n",
    "    for frac, nrmse in nrmse_values.items():\n",
    "        print(f\"  Missing Fraction {frac * 100:.0f}%: NRMSE = {nrmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to tabular_transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"tabular_transformer_model.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
