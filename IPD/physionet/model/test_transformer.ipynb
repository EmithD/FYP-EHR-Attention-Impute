{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tensor Shape: torch.Size([1606, 38])\n",
      "Test Tensor Shape: torch.Size([402, 38])\n",
      "Device Used: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('../data/finalEDA/data/dataset_1_39_features.csv', index_col=0)\n",
    "data = df.to_numpy()\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32).to(device)\n",
    "test_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Verify Shapes\n",
    "print(f\"Train Tensor Shape: {train_tensor.shape}\")\n",
    "print(f\"Test Tensor Shape: {test_tensor.shape}\")\n",
    "print(f\"Device Used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1606, 38])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, d_model=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Linear(1, d_model)  # Embed each feature (column) into d_model dimensions\n",
    "\n",
    "        self.column_embedding = nn.Embedding(num_features, d_model)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=128, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, 1)  # Predict one value per feature\n",
    "\n",
    "    def forward(self, x, column_indices, mask=None):\n",
    "\n",
    "        x = x.unsqueeze(-1)\n",
    "\n",
    "        x_embed = self.embedding(x)\n",
    "\n",
    "        column_embed = self.column_embedding(column_indices)\n",
    "        x_embed += column_embed.unsqueeze(0)\n",
    "\n",
    "        x_encoded = self.transformer_encoder(x_embed, mask=mask)\n",
    "\n",
    "        output = self.output_layer(x_encoded)\n",
    "        return output.squeeze(-1)\n",
    "\n",
    "num_features = train_tensor.shape[1]\n",
    "model = TransformerModel(num_features=num_features, d_model=64, num_heads=4, num_layers=2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "column_indices = torch.arange(num_features).to(device)\n",
    "\n",
    "output = model(train_tensor, column_indices)\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Shape: torch.Size([1606, 38])\n"
     ]
    }
   ],
   "source": [
    "def create_missing_mask(data, missing_fraction=0.2):\n",
    "    mask = torch.rand(data.shape).to(data.device) < missing_fraction\n",
    "    return mask.int()\n",
    "\n",
    "missing_fraction = 0.2\n",
    "mask = create_missing_mask(train_tensor, missing_fraction)\n",
    "print(f\"Mask Shape: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, ground_truth, mask):\n",
    "    mse_loss = nn.MSELoss(reduction='none')\n",
    "    loss = mse_loss(predictions, ground_truth)\n",
    "    masked_loss = (loss * mask).sum() / mask.sum()  # Normalize by number of masked positions\n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: nan\n",
      "Epoch 2/20, Loss: nan\n",
      "Epoch 3/20, Loss: nan\n",
      "Epoch 4/20, Loss: nan\n",
      "Epoch 5/20, Loss: nan\n",
      "Epoch 6/20, Loss: nan\n",
      "Epoch 7/20, Loss: nan\n",
      "Epoch 8/20, Loss: nan\n",
      "Epoch 9/20, Loss: nan\n",
      "Epoch 10/20, Loss: nan\n",
      "Epoch 11/20, Loss: nan\n",
      "Epoch 12/20, Loss: nan\n",
      "Epoch 13/20, Loss: nan\n",
      "Epoch 14/20, Loss: nan\n",
      "Epoch 15/20, Loss: nan\n",
      "Epoch 16/20, Loss: nan\n",
      "Epoch 17/20, Loss: nan\n",
      "Epoch 18/20, Loss: nan\n",
      "Epoch 19/20, Loss: nan\n",
      "Epoch 20/20, Loss: nan\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    mask = create_missing_mask(train_tensor, missing_fraction)\n",
    "    input_with_mask = train_tensor.clone()\n",
    "    input_with_mask[mask == 1] = 0\n",
    "\n",
    "    predictions = model(input_with_mask, column_indices)\n",
    "\n",
    "    loss = compute_loss(predictions, train_tensor, mask)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(nan, device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_data, column_indices, missing_fraction=0.2):\n",
    "    model.eval()\n",
    "\n",
    "    mask = create_missing_mask(test_data, missing_fraction)\n",
    "    input_with_mask = test_data.clone()\n",
    "    input_with_mask[mask == 1] = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "\n",
    "    loss = compute_loss(predictions, test_data, mask)\n",
    "    print(f\"Test Loss: {loss.item():.4f}\")\n",
    "    return loss\n",
    "\n",
    "evaluate_model(model, test_tensor, column_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE: nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()\n",
    "\n",
    "def evaluate_model_with_nrmse(model, test_data, column_indices, missing_fraction=0.2):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    mask = create_missing_mask(test_data, missing_fraction)\n",
    "    input_with_mask = test_data.clone()\n",
    "    input_with_mask[mask == 1] = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "\n",
    "    nrmse = compute_nrmse(predictions, test_data, mask)\n",
    "    print(f\"NRMSE: {nrmse:.4f}\")\n",
    "    return nrmse\n",
    "\n",
    "nrmse = evaluate_model_with_nrmse(model, test_tensor, column_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyampute.ampute import MultivariateAmputation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def create_missing_dataset(data, missing_fraction=0.1, mechanism=\"MCAR\"):\n",
    "\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = pd.DataFrame(data)\n",
    "\n",
    "    patterns = [{\n",
    "        \"incomplete_vars\": data.columns.tolist(),\n",
    "        \"weights\": np.zeros(len(data.columns)),  # Default for MCAR\n",
    "        \"mechanism\": mechanism,\n",
    "        \"score_to_probability_func\": \"sigmoid-right\"\n",
    "    }]\n",
    "\n",
    "    if mechanism == \"MAR\":\n",
    "        num_columns = len(data.columns)\n",
    "        num_amputed_columns = max(1, int(num_columns * 0.5))\n",
    "        amputed_columns = np.random.choice(data.columns, num_amputed_columns, replace=False)\n",
    "        patterns[0][\"incomplete_vars\"] = amputed_columns\n",
    "        patterns[0][\"weights\"] = np.random.uniform(-1, 1, num_columns) \n",
    "    \n",
    "    elif mechanism == \"MNAR\":\n",
    "        patterns[0][\"weights\"] = np.random.uniform(0.5, 2, len(data.columns))\n",
    "\n",
    "    amputer = MultivariateAmputation(prop=missing_fraction, patterns=patterns)\n",
    "    amputed_data = amputer.fit_transform(data)\n",
    "\n",
    "    mask = pd.isna(amputed_data).astype(int).to_numpy()\n",
    "    amputed_data = np.nan_to_num(amputed_data, nan=0.0)\n",
    "\n",
    "    amputed_data = torch.tensor(amputed_data, dtype=torch.float32).to(device)\n",
    "    mask = torch.tensor(mask, dtype=torch.int32).to(device)\n",
    "\n",
    "    assert amputed_data.shape == mask.shape, \"Data and mask shapes dont match\"\n",
    "    return amputed_data, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_missingness(model, original_data, column_indices, missing_fraction=0.1, mechanism=\"MCAR\"):\n",
    "    model.eval()\n",
    "\n",
    "    amputed_data, mask = create_missing_dataset(original_data.cpu().numpy(), missing_fraction, mechanism)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(amputed_data, column_indices)\n",
    "\n",
    "    nrmse = compute_nrmse(predictions, original_data, mask)\n",
    "    print(f\"{mechanism} NRMSE at {missing_fraction * 100:.0f}% Missing: {nrmse:.4f}\")\n",
    "    return nrmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_across_mechanisms(model, original_data, column_indices, missing_fractions):\n",
    "    mechanisms = [\"MCAR\", \"MAR\", \"MNAR\"]\n",
    "    results = {mechanism: {} for mechanism in mechanisms}\n",
    "    \n",
    "    for mechanism in mechanisms:\n",
    "        for fraction in missing_fractions:\n",
    "            results[mechanism][fraction] = evaluate_model_with_missingness(\n",
    "                model, original_data, column_indices, missing_fraction=fraction, mechanism=mechanism\n",
    "            )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 22:14:57,377 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-03-29 22:14:57,394 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-03-29 22:14:57,403 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-03-29 22:14:57,413 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-03-29 22:14:57,423 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-03-29 22:14:57,432 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-03-29 22:14:57,433 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAR NRMSE at 10% Missing: nan\n",
      "MCAR NRMSE at 20% Missing: nan\n",
      "MCAR NRMSE at 30% Missing: nan\n",
      "MCAR NRMSE at 40% Missing: nan\n",
      "MCAR NRMSE at 50% Missing: nan\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Features involved in amputation must be complete, but contains NaNs.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m missing_fractions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_across_mechanisms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_fractions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mechanism, nrmse_values \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmechanism\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Results:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 7\u001b[0m, in \u001b[0;36mevaluate_across_mechanisms\u001b[1;34m(model, original_data, column_indices, missing_fractions)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mechanism \u001b[38;5;129;01min\u001b[39;00m mechanisms:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fraction \u001b[38;5;129;01min\u001b[39;00m missing_fractions:\n\u001b[1;32m----> 7\u001b[0m         results[mechanism][fraction] \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model_with_missingness\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moriginal_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_fraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfraction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmechanism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmechanism\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m, in \u001b[0;36mevaluate_model_with_missingness\u001b[1;34m(model, original_data, column_indices, missing_fraction, mechanism)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_model_with_missingness\u001b[39m(model, original_data, column_indices, missing_fraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, mechanism\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMCAR\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m----> 4\u001b[0m     amputed_data, mask \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_missing_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_fraction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmechanism\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      7\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m model(amputed_data, column_indices)\n",
      "Cell \u001b[1;32mIn[9], line 29\u001b[0m, in \u001b[0;36mcreate_missing_dataset\u001b[1;34m(data, missing_fraction, mechanism)\u001b[0m\n\u001b[0;32m     26\u001b[0m     patterns[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m     28\u001b[0m amputer \u001b[38;5;241m=\u001b[39m MultivariateAmputation(prop\u001b[38;5;241m=\u001b[39mmissing_fraction, patterns\u001b[38;5;241m=\u001b[39mpatterns)\n\u001b[1;32m---> 29\u001b[0m amputed_data \u001b[38;5;241m=\u001b[39m \u001b[43mamputer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m mask \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39misna(amputed_data)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[0;32m     32\u001b[0m amputed_data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan_to_num(amputed_data, nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:926\u001b[0m, in \u001b[0;36mMultivariateAmputation.transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwss_per_pattern \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    924\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs_per_pattern \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 926\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    927\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    929\u001b[0m \u001b[38;5;66;03m# split complete_data in groups\u001b[39;00m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;66;03m# the number of groups is defined by the number of patterns\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:769\u001b[0m, in \u001b[0;36mMultivariateAmputation._validate_data\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[38;5;66;03m# enforce numpy just for checking\u001b[39;00m\n\u001b[0;32m    768\u001b[0m X_check \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(X, DataFrame) \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 769\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isnull(\n\u001b[0;32m    770\u001b[0m     X_check[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvars_involved_in_ampute]\n\u001b[0;32m    771\u001b[0m )\u001b[38;5;241m.\u001b[39many(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures involved in amputation must be complete, but contains NaNs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_numeric(X_check[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvars_involved_in_ampute]):\n\u001b[0;32m    773\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    774\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeatures involved in amputation found to be non-numeric.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    775\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m They will be forced to numeric upon calculating sum scores.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    776\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Features involved in amputation must be complete, but contains NaNs."
     ]
    }
   ],
   "source": [
    "missing_fractions = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "results = evaluate_across_mechanisms(model, test_tensor, column_indices, missing_fractions)\n",
    "\n",
    "for mechanism, nrmse_values in results.items():\n",
    "    print(f\"\\n{mechanism} Results:\")\n",
    "    for frac, nrmse in nrmse_values.items():\n",
    "        print(f\"  Missing Fraction {frac * 100:.0f}%: NRMSE = {nrmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to tabular_transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_path = \"tabular_transformer_model.pth\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 04:03:24,105 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,118 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,127 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,137 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,148 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,157 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,158 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:24,159 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,168 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,176 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,177 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:24,178 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,186 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,194 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,195 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:24,196 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,205 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,212 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,213 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:24,215 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,223 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,231 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,232 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-02-28 04:03:24,233 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,241 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,249 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,250 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,258 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,265 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,267 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,273 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,281 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,282 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,289 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,296 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,298 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,306 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n",
      "2025-02-28 04:03:24,313 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-02-28 04:03:24,315 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAR NRMSE at 10% Missing: 0.0508\n",
      "MCAR NRMSE at 20% Missing: 0.0505\n",
      "MCAR NRMSE at 30% Missing: 0.0482\n",
      "MCAR NRMSE at 40% Missing: 0.0477\n",
      "MCAR NRMSE at 50% Missing: 0.0478\n",
      "MAR NRMSE at 10% Missing: 0.0431\n",
      "MAR NRMSE at 20% Missing: 0.0514\n",
      "MAR NRMSE at 30% Missing: 0.0496\n",
      "MAR NRMSE at 40% Missing: 0.0414\n",
      "MAR NRMSE at 50% Missing: 0.0590\n",
      "MNAR NRMSE at 10% Missing: 0.0603\n",
      "MNAR NRMSE at 20% Missing: 0.0521\n",
      "MNAR NRMSE at 30% Missing: 0.0480\n",
      "MNAR NRMSE at 40% Missing: 0.0466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyampute\\ampute.py:400: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  data_group = stats.zscore(data_group)\n",
      "2025-02-28 04:03:24,322 [WARNING] Candidates for pattern 0 all have almost the same weighted sum scores. It is possible this is due to the use of binary variables in amputation. This creates problems when using the sigmoid function for the score_to_probability_func. Currently our solution is as follows: if there is just one candidate with a sum score 0, we will ampute it. If there is one candidate with a nonzero sum score, or multiple candidates with the same score, we evenly apply as if MCAR.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNAR NRMSE at 50% Missing: 0.0470\n",
      "\n",
      "MCAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0508\n",
      "  Missing Fraction 20%: NRMSE = 0.0505\n",
      "  Missing Fraction 30%: NRMSE = 0.0482\n",
      "  Missing Fraction 40%: NRMSE = 0.0477\n",
      "  Missing Fraction 50%: NRMSE = 0.0478\n",
      "\n",
      "MAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0431\n",
      "  Missing Fraction 20%: NRMSE = 0.0514\n",
      "  Missing Fraction 30%: NRMSE = 0.0496\n",
      "  Missing Fraction 40%: NRMSE = 0.0414\n",
      "  Missing Fraction 50%: NRMSE = 0.0590\n",
      "\n",
      "MNAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0603\n",
      "  Missing Fraction 20%: NRMSE = 0.0521\n",
      "  Missing Fraction 30%: NRMSE = 0.0480\n",
      "  Missing Fraction 40%: NRMSE = 0.0466\n",
      "  Missing Fraction 50%: NRMSE = 0.0470\n"
     ]
    }
   ],
   "source": [
    "new_df = pd.read_csv('synthetic_physionet_data.csv', index_col=0)\n",
    "new_data = new_df.to_numpy()\n",
    "\n",
    "new_test_data = scaler.transform(new_data)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "new_test_tensor = torch.tensor(new_test_data, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "new_results = evaluate_across_mechanisms(model, new_test_tensor, column_indices, missing_fractions)\n",
    "\n",
    "for mechanism, nrmse_values in new_results.items():\n",
    "    print(f\"\\n{mechanism} Results:\")\n",
    "    for frac, nrmse in nrmse_values.items():\n",
    "        print(f\"  Missing Fraction {frac * 100:.0f}%: NRMSE = {nrmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
