{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tensor Shape: torch.Size([1278, 39])\n",
      "Test Tensor Shape: torch.Size([320, 39])\n",
      "Device Used: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "\n",
    "# Load Dataset\n",
    "# Replace 'dataset.csv' with your dataset file path\n",
    "df = pd.read_csv('../data/physionet_wo_missing.csv', index_col=0)\n",
    "\n",
    "# Convert dataset to numpy array (assuming all features are numeric)\n",
    "data = df.to_numpy()\n",
    "\n",
    "# Split Dataset\n",
    "# Specify the test size (e.g., 20% for testing)\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize Data\n",
    "# Use StandardScaler to normalize the features\n",
    "scaler = StandardScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "# Convert to Torch Tensors\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_tensor = torch.tensor(train_data, dtype=torch.float32).to(device)\n",
    "test_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "\n",
    "# Verify Shapes\n",
    "print(f\"Train Tensor Shape: {train_tensor.shape}\")\n",
    "print(f\"Test Tensor Shape: {test_tensor.shape}\")\n",
    "print(f\"Device Used: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1278, 39])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emith\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_features, d_model=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(TabularTransformer, self).__init__()\n",
    "        \n",
    "        # 1) Column Embedding Layer\n",
    "        self.embedding = nn.Linear(1, d_model)  # Embed each feature (column) into d_model dimensions\n",
    "        \n",
    "        # 2) Positional Encoding (Column Embedding)\n",
    "        self.column_embedding = nn.Embedding(num_features, d_model)\n",
    "        \n",
    "        # 3) Transformer Encoder Layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=128, \n",
    "            dropout=dropout,\n",
    "            batch_first=True  # Use batch-first format for better performance\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # 4) Output Layer\n",
    "        self.output_layer = nn.Linear(d_model, 1)  # Predict one value per feature\n",
    "\n",
    "    def forward(self, x, column_indices, mask=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, num_features]\n",
    "        column_indices: Tensor of column indices for embedding (shape: [num_features])\n",
    "        mask: Optional mask for self-attention (shape: [batch_size, num_features, num_features])\n",
    "        \"\"\"\n",
    "        # Reshape x to [batch_size, num_features, 1] for embedding\n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        # Column Embedding\n",
    "        x_embed = self.embedding(x)\n",
    "        \n",
    "        # Add Positional (Column) Embedding\n",
    "        column_embed = self.column_embedding(column_indices)\n",
    "        x_embed += column_embed.unsqueeze(0)  # Broadcast across batch\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        x_encoded = self.transformer_encoder(x_embed, mask=mask)\n",
    "        \n",
    "        # Final Output Layer (Predict missing values)\n",
    "        output = self.output_layer(x_encoded)\n",
    "        return output.squeeze(-1)  # [batch_size, num_features]\n",
    "\n",
    "# Model Initialization\n",
    "num_features = train_tensor.shape[1]  # Number of features (columns in dataset)\n",
    "model = TabularTransformer(num_features=num_features, d_model=64, num_heads=4, num_layers=2)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# Define Column Indices\n",
    "column_indices = torch.arange(num_features).to(device)\n",
    "\n",
    "# Example Forward Pass\n",
    "output = model(train_tensor, column_indices)\n",
    "print(f\"Output shape: {output.shape}\")  # Should match [batch_size, num_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask Shape: torch.Size([1278, 39])\n"
     ]
    }
   ],
   "source": [
    "def create_missing_mask(data, missing_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Creates a binary mask for missing values.\n",
    "\n",
    "    Args:\n",
    "        data (Tensor): Input data (shape: [batch_size, num_features]).\n",
    "        missing_fraction (float): Fraction of values to mask.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Binary mask of the same shape as `data` (1 = missing, 0 = observed).\n",
    "    \"\"\"\n",
    "    mask = torch.rand(data.shape).to(data.device) < missing_fraction\n",
    "    return mask.int()\n",
    "\n",
    "# Example usage\n",
    "missing_fraction = 0.2  # Mask 20% of the data\n",
    "mask = create_missing_mask(train_tensor, missing_fraction)\n",
    "print(f\"Mask Shape: {mask.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Computes the loss only on masked positions.\n",
    "\n",
    "    Args:\n",
    "        predictions (Tensor): Model predictions (shape: [batch_size, num_features]).\n",
    "        ground_truth (Tensor): Original data (shape: [batch_size, num_features]).\n",
    "        mask (Tensor): Binary mask (1 = missing, 0 = observed).\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Scalar loss value.\n",
    "    \"\"\"\n",
    "    mse_loss = nn.MSELoss(reduction='none')\n",
    "    loss = mse_loss(predictions, ground_truth)\n",
    "    masked_loss = (loss * mask).sum() / mask.sum()  # Normalize by number of masked positions\n",
    "    return masked_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Loss: 1.2102\n",
      "Epoch 2/20, Loss: 1.2609\n",
      "Epoch 3/20, Loss: 1.1221\n",
      "Epoch 4/20, Loss: 1.0523\n",
      "Epoch 5/20, Loss: 1.0262\n",
      "Epoch 6/20, Loss: 0.9926\n",
      "Epoch 7/20, Loss: 0.9286\n",
      "Epoch 8/20, Loss: 1.0039\n",
      "Epoch 9/20, Loss: 1.0599\n",
      "Epoch 10/20, Loss: 1.0303\n",
      "Epoch 11/20, Loss: 0.9674\n",
      "Epoch 12/20, Loss: 1.0186\n",
      "Epoch 13/20, Loss: 1.0311\n",
      "Epoch 14/20, Loss: 0.9629\n",
      "Epoch 15/20, Loss: 1.1192\n",
      "Epoch 16/20, Loss: 1.0817\n",
      "Epoch 17/20, Loss: 1.0180\n",
      "Epoch 18/20, Loss: 0.9703\n",
      "Epoch 19/20, Loss: 0.9981\n",
      "Epoch 20/20, Loss: 1.1161\n"
     ]
    }
   ],
   "source": [
    "# Define Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Generate masks and replace masked values with 0\n",
    "    mask = create_missing_mask(train_tensor, missing_fraction)\n",
    "    input_with_mask = train_tensor.clone()\n",
    "    input_with_mask[mask == 1] = 0  # Replace masked positions with 0\n",
    "    \n",
    "    # Forward Pass\n",
    "    predictions = model(input_with_mask, column_indices)\n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = compute_loss(predictions, train_tensor, mask)\n",
    "    \n",
    "    # Backward Pass and Optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0148, device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_data, column_indices, missing_fraction=0.2):\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate masks and replace masked values with 0\n",
    "    mask = create_missing_mask(test_data, missing_fraction)\n",
    "    input_with_mask = test_data.clone()\n",
    "    input_with_mask[mask == 1] = 0  # Replace masked positions with 0\n",
    "    \n",
    "    # Forward Pass\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = compute_loss(predictions, test_data, mask)\n",
    "    print(f\"Test Loss: {loss.item():.4f}\")\n",
    "    return loss\n",
    "\n",
    "# Evaluate the Model\n",
    "evaluate_model(model, test_tensor, column_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSE: 0.0271\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Computes the Normalized Root Mean Squared Error (NRMSE) for imputed values.\n",
    "\n",
    "    Args:\n",
    "        predictions (Tensor): Model predictions (shape: [batch_size, num_features]).\n",
    "        ground_truth (Tensor): Original data (shape: [batch_size, num_features]).\n",
    "        mask (Tensor): Binary mask (1 = missing, 0 = observed).\n",
    "\n",
    "    Returns:\n",
    "        float: NRMSE value for the masked positions.\n",
    "    \"\"\"\n",
    "    # Extract only masked values\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "    \n",
    "    # Compute RMSE\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    # Compute range of ground truth values\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "    \n",
    "    # Normalize RMSE\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()\n",
    "\n",
    "def evaluate_model_with_nrmse(model, test_data, column_indices, missing_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Evaluates the model and computes NRMSE for masked positions.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        test_data (Tensor): Test dataset (shape: [batch_size, num_features]).\n",
    "        column_indices (Tensor): Column indices for embedding (shape: [num_features]).\n",
    "        missing_fraction (float): Fraction of values to mask.\n",
    "\n",
    "    Returns:\n",
    "        float: NRMSE value for the test dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate masks and replace masked values with 0\n",
    "    mask = create_missing_mask(test_data, missing_fraction)\n",
    "    input_with_mask = test_data.clone()\n",
    "    input_with_mask[mask == 1] = 0  # Replace masked positions with 0\n",
    "    \n",
    "    # Forward Pass\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "    \n",
    "    # Compute NRMSE\n",
    "    nrmse = compute_nrmse(predictions, test_data, mask)\n",
    "    print(f\"NRMSE: {nrmse:.4f}\")\n",
    "    return nrmse\n",
    "\n",
    "# Evaluate the Model with NRMSE\n",
    "nrmse = evaluate_model_with_nrmse(model, test_tensor, column_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyampute.ampute import MultivariateAmputation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Utility to create missing datasets (MCAR, MAR, MNAR)\n",
    "def create_missing_dataset(data, missing_fraction=0.1, mechanism=\"MCAR\"):\n",
    "    \"\"\"\n",
    "    Introduces missingness (MCAR, MAR, MNAR) into the dataset and generates a mask.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray or pd.DataFrame): Original dataset (numerical).\n",
    "        missing_fraction (float): Fraction of missing values to introduce.\n",
    "        mechanism (str): Missingness mechanism (\"MCAR\", \"MAR\", or \"MNAR\").\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Data with missing values replaced by 0.0.\n",
    "        Tensor: Binary mask where 1 = missing, 0 = observed.\n",
    "    \"\"\"\n",
    "    if isinstance(data, np.ndarray):\n",
    "        data = pd.DataFrame(data)\n",
    "    \n",
    "    # Define the amputation pattern\n",
    "    patterns = [{\n",
    "        \"incomplete_vars\": data.columns.tolist(),\n",
    "        \"weights\": np.zeros(len(data.columns)),  # Default for MCAR\n",
    "        \"mechanism\": mechanism,\n",
    "        \"score_to_probability_func\": \"sigmoid-right\"\n",
    "    }]\n",
    "    \n",
    "    # Customize for MAR or MNAR\n",
    "    if mechanism == \"MAR\":\n",
    "        # Randomly choose a subset of columns for MAR\n",
    "        num_columns = len(data.columns)\n",
    "        num_amputed_columns = max(1, int(num_columns * 0.5))  # Amputate 50% of columns\n",
    "        amputed_columns = np.random.choice(data.columns, num_amputed_columns, replace=False)\n",
    "        patterns[0][\"incomplete_vars\"] = amputed_columns\n",
    "        patterns[0][\"weights\"] = np.random.uniform(-1, 1, num_columns)  # Random weights\n",
    "    \n",
    "    elif mechanism == \"MNAR\":\n",
    "        patterns[0][\"weights\"] = np.random.uniform(0.5, 2, len(data.columns))  # Skewed weights\n",
    "\n",
    "    # Apply missingness with pyampute\n",
    "    amputer = MultivariateAmputation(prop=missing_fraction, patterns=patterns)\n",
    "    amputed_data = amputer.fit_transform(data)\n",
    "    \n",
    "    # Create mask and fill NaNs with 0.0\n",
    "    mask = pd.isna(amputed_data).astype(int).to_numpy()\n",
    "    amputed_data = np.nan_to_num(amputed_data, nan=0.0)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    amputed_data = torch.tensor(amputed_data, dtype=torch.float32).to(device)\n",
    "    mask = torch.tensor(mask, dtype=torch.int32).to(device)\n",
    "    \n",
    "    # Ensure shapes match\n",
    "    assert amputed_data.shape == mask.shape, \"Data and mask shapes do not match!\"\n",
    "    return amputed_data, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Compute NRMSE\n",
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Computes the Normalized Root Mean Squared Error (NRMSE) for imputed values.\n",
    "\n",
    "    Args:\n",
    "        predictions (Tensor): Model predictions (shape: [batch_size, num_features]).\n",
    "        ground_truth (Tensor): Original data (shape: [batch_size, num_features]).\n",
    "        mask (Tensor): Binary mask (1 = missing, 0 = observed).\n",
    "\n",
    "    Returns:\n",
    "        float: NRMSE value for the masked positions.\n",
    "    \"\"\"\n",
    "    # Extract only masked values\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "    \n",
    "    # Compute RMSE\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    # Compute range of ground truth values\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "    \n",
    "    # Normalize RMSE\n",
    "    nrmse = rmse / data_range\n",
    "    return nrmse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model with missing data\n",
    "def evaluate_model_with_missingness(model, original_data, column_indices, missing_fraction=0.1, mechanism=\"MCAR\"):\n",
    "    \"\"\"\n",
    "    Evaluates the model using a dataset with missing data (MCAR, MAR, or MNAR).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        original_data (Tensor): Original dataset (shape: [batch_size, num_features]).\n",
    "        column_indices (Tensor): Column indices for embedding (shape: [num_features]).\n",
    "        missing_fraction (float): Fraction of missing values to introduce.\n",
    "        mechanism (str): Missingness mechanism (\"MCAR\", \"MAR\", or \"MNAR\").\n",
    "\n",
    "    Returns:\n",
    "        float: NRMSE value for the dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate dataset with missing data\n",
    "    amputed_data, mask = create_missing_dataset(original_data.cpu().numpy(), missing_fraction, mechanism)\n",
    "    \n",
    "    # Forward pass with the amputed data\n",
    "    with torch.no_grad():\n",
    "        predictions = model(amputed_data, column_indices)\n",
    "    \n",
    "    # Compute NRMSE\n",
    "    nrmse = compute_nrmse(predictions, original_data, mask)\n",
    "    print(f\"{mechanism} NRMSE at {missing_fraction * 100:.0f}% Missing: {nrmse:.4f}\")\n",
    "    return nrmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate for all mechanisms\n",
    "def evaluate_across_mechanisms(model, original_data, column_indices, missing_fractions):\n",
    "    \"\"\"\n",
    "    Evaluates the model for MCAR, MAR, and MNAR at different missing fractions.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Trained model.\n",
    "        original_data (Tensor): Original dataset.\n",
    "        column_indices (Tensor): Column indices for embedding.\n",
    "        missing_fractions (list): List of missing fractions to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results of NRMSE for each mechanism and fraction.\n",
    "    \"\"\"\n",
    "    mechanisms = [\"MCAR\", \"MAR\", \"MNAR\"]\n",
    "    results = {mechanism: {} for mechanism in mechanisms}\n",
    "    \n",
    "    for mechanism in mechanisms:\n",
    "        for fraction in missing_fractions:\n",
    "            results[mechanism][fraction] = evaluate_model_with_missingness(\n",
    "                model, original_data, column_indices, missing_fraction=fraction, mechanism=mechanism\n",
    "            )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 23:42:45,871 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:45,883 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:45,897 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:45,912 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:45,923 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:45,934 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:45,935 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-01-28 23:42:45,937 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-01-28 23:42:45,960 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:45,961 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-01-28 23:42:45,963 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-01-28 23:42:45,982 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:45,983 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-01-28 23:42:45,984 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-01-28 23:42:46,003 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:46,004 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-01-28 23:42:46,005 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-01-28 23:42:46,027 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:46,027 [WARNING] Indicated weights for incomplete vars for a pattern with MAR. Did you mean MAR+MNAR?\n",
      "2025-01-28 23:42:46,029 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-01-28 23:42:46,048 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:46,051 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-01-28 23:42:46,068 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAR NRMSE at 10% Missing: 0.0338\n",
      "MCAR NRMSE at 20% Missing: 0.0305\n",
      "MCAR NRMSE at 30% Missing: 0.0262\n",
      "MCAR NRMSE at 40% Missing: 0.0288\n",
      "MCAR NRMSE at 50% Missing: 0.0273\n",
      "MAR NRMSE at 10% Missing: 0.0299\n",
      "MAR NRMSE at 20% Missing: 0.0210\n",
      "MAR NRMSE at 30% Missing: 0.0338\n",
      "MAR NRMSE at 40% Missing: 0.0266\n",
      "MAR NRMSE at 50% Missing: 0.0268\n",
      "MNAR NRMSE at 10% Missing: 0.0306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-28 23:42:46,070 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-01-28 23:42:46,090 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:46,092 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-01-28 23:42:46,114 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:46,116 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n",
      "2025-01-28 23:42:46,134 [WARNING] Failed to load lookup table for a prespecified score to probability function. It is possible data\\shift_lookup.csv is missing, in the wrong location, or corrupted. Try rerunning scripts/generate_shift_lookup_table.py to regenerate the lookup table.\n",
      "2025-01-28 23:42:46,136 [WARNING] Binary variables (at indices [ 0  1  2  3  4  5  7 10 12 24]) are indicated to be used in amputation (they are weighted and will be used to calculate the weighted sum score under MAR, MNAR, or MAR+MNAR). This can result in a subset with candidates that all have the same (or almost the same) weighted sum scores. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNAR NRMSE at 20% Missing: 0.0337\n",
      "MNAR NRMSE at 30% Missing: 0.0326\n",
      "MNAR NRMSE at 40% Missing: 0.0303\n",
      "MNAR NRMSE at 50% Missing: 0.0307\n",
      "\n",
      "MCAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0338\n",
      "  Missing Fraction 20%: NRMSE = 0.0305\n",
      "  Missing Fraction 30%: NRMSE = 0.0262\n",
      "  Missing Fraction 40%: NRMSE = 0.0288\n",
      "  Missing Fraction 50%: NRMSE = 0.0273\n",
      "\n",
      "MAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0299\n",
      "  Missing Fraction 20%: NRMSE = 0.0210\n",
      "  Missing Fraction 30%: NRMSE = 0.0338\n",
      "  Missing Fraction 40%: NRMSE = 0.0266\n",
      "  Missing Fraction 50%: NRMSE = 0.0268\n",
      "\n",
      "MNAR Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0306\n",
      "  Missing Fraction 20%: NRMSE = 0.0337\n",
      "  Missing Fraction 30%: NRMSE = 0.0326\n",
      "  Missing Fraction 40%: NRMSE = 0.0303\n",
      "  Missing Fraction 50%: NRMSE = 0.0307\n"
     ]
    }
   ],
   "source": [
    "# Define missing fractions to test\n",
    "missing_fractions = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "# Run evaluations for MCAR, MAR, and MNAR\n",
    "results = evaluate_across_mechanisms(model, test_tensor, column_indices, missing_fractions)\n",
    "\n",
    "# Print or log results\n",
    "for mechanism, nrmse_values in results.items():\n",
    "    print(f\"\\n{mechanism} Results:\")\n",
    "    for frac, nrmse in nrmse_values.items():\n",
    "        print(f\"  Missing Fraction {frac * 100:.0f}%: NRMSE = {nrmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to tabular_transformer_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Path to save the model\n",
    "model_path = \"tabular_transformer_model.pth\"\n",
    "\n",
    "# Save the model's state dictionary (recommended)\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
