{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading + Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, index_col=0):\n",
    "    \"\"\"\n",
    "    Load data from CSV file and handle potential errors.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the CSV file\n",
    "        index_col (int, optional): Column to use as index. Defaults to 0.\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Loaded data as numpy array\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_path = Path(file_path)\n",
    "        if not file_path.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "            \n",
    "        logger.info(f\"Loading data from {file_path}\")\n",
    "        df = pd.read_csv(file_path, index_col=index_col)\n",
    "        \n",
    "        # Basic data validation\n",
    "        if df.empty:\n",
    "            raise ValueError(\"The loaded DataFrame is empty\")\n",
    "        \n",
    "        # Check for remaining missing values\n",
    "        missing_values = df.isnull().sum().sum()\n",
    "        if missing_values > 0:\n",
    "            logger.warning(f\"Dataset contains {missing_values} missing values\")\n",
    "            \n",
    "        # Log data shape information\n",
    "        logger.info(f\"Loaded data shape: {df.shape}\")\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        data = df.to_numpy()\n",
    "        return data\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_training(data, test_size=0.2, random_state=42, device=None):\n",
    "    \"\"\"\n",
    "    Split data into train and test sets, scale, and convert to tensors.\n",
    "    \n",
    "    Args:\n",
    "        data (numpy.ndarray): Input data\n",
    "        test_size (float, optional): Proportion of test set. Defaults to 0.2.\n",
    "        random_state (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        device (torch.device, optional): Device to use. Defaults to None.\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_tensor, test_tensor, scaler) - PyTorch tensors and scaler\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine device to use\n",
    "        if device is None:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        logger.info(f\"Using device: {device}\")\n",
    "        \n",
    "        # Split data into train and test sets\n",
    "        train_data, test_data = train_test_split(\n",
    "            data, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        logger.info(f\"Train set shape: {train_data.shape}, Test set shape: {test_data.shape}\")\n",
    "        \n",
    "        # Scale data\n",
    "        scaler = StandardScaler()\n",
    "        train_data = scaler.fit_transform(train_data)\n",
    "        test_data = scaler.transform(test_data)\n",
    "        \n",
    "        # Check for NaNs or infinities after scaling\n",
    "        if np.isnan(train_data).any() or np.isinf(train_data).any():\n",
    "            logger.warning(\"Train data contains NaN or infinite values after scaling\")\n",
    "            \n",
    "        if np.isnan(test_data).any() or np.isinf(test_data).any():\n",
    "            logger.warning(\"Test data contains NaN or infinite values after scaling\")\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        train_tensor = torch.tensor(train_data, dtype=torch.float32).to(device)\n",
    "        test_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "        \n",
    "        logger.info(f\"Train tensor shape: {train_tensor.shape}\")\n",
    "        logger.info(f\"Test tensor shape: {test_tensor.shape}\")\n",
    "        \n",
    "        return train_tensor, test_tensor, scaler\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preparing data: {str(e)}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-29 16:18:42,605 - __main__ - INFO - Loading data from ..\\data\\physionet_wo_missing.csv\n",
      "2025-03-29 16:18:42,613 - __main__ - INFO - Loaded data shape: (1598, 39)\n",
      "2025-03-29 16:18:42,614 - __main__ - INFO - Using device: cuda\n",
      "2025-03-29 16:18:42,615 - __main__ - INFO - Train set shape: (1278, 39), Test set shape: (320, 39)\n",
      "2025-03-29 16:18:42,618 - __main__ - INFO - Train tensor shape: torch.Size([1278, 39])\n",
      "2025-03-29 16:18:42,618 - __main__ - INFO - Test tensor shape: torch.Size([320, 39])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Tensor Shape: torch.Size([1278, 39])\n",
      "Test Tensor Shape: torch.Size([320, 39])\n",
      "Device Used: cuda\n"
     ]
    }
   ],
   "source": [
    "# File path with more robust handling\n",
    "file_path = os.path.join('..', 'data', 'physionet_wo_missing.csv')\n",
    "\n",
    "# Load data\n",
    "data = load_data(file_path)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Prepare data\n",
    "train_tensor, test_tensor, scaler = prepare_data_for_training(\n",
    "    data,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Verify shapes\n",
    "print(f\"Train Tensor Shape: {train_tensor.shape}\")\n",
    "print(f\"Test Tensor Shape: {test_tensor.shape}\")\n",
    "print(f\"Device Used: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional encoding to provide position information to the transformer model.\n",
    "    This helps the model understand the relative positions of features.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len=1000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Wise attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureWiseAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Feature-wise attention mechanism to help the model focus on important relationships\n",
    "    between different features during imputation.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model):\n",
    "        super(FeatureWiseAttention, self).__init__()\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / self.scale\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attn, v)\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, d_model=128, num_heads=8, num_layers=3, dropout=0.2, \n",
    "                 use_layer_norm=True, feedforward_dim=512, activation='gelu'):\n",
    "        super(EnhancedTransformerModel, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_features = num_features\n",
    "        \n",
    "        # Enhanced feature embedding with layer normalization\n",
    "        self.feature_embedding = nn.Sequential(\n",
    "            nn.Linear(1, d_model),\n",
    "            nn.LayerNorm(d_model) if use_layer_norm else nn.Identity(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Improved column embedding with additional context\n",
    "        self.column_embedding = nn.Embedding(num_features, d_model)\n",
    "        \n",
    "        # Add positional encoding to provide position information\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Select activation function\n",
    "        if activation == 'gelu':\n",
    "            activation_fn = nn.GELU()\n",
    "        elif activation == 'relu':\n",
    "            activation_fn = nn.ReLU()\n",
    "        else:\n",
    "            activation_fn = nn.GELU()  # Default to GELU\n",
    "        \n",
    "        # Enhanced transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=feedforward_dim,\n",
    "            dropout=dropout,\n",
    "            activation=activation_fn,\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Apply normalization before attention & feedforward\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        # Feature-wise attention to focus on important relationships\n",
    "        self.feature_attention = FeatureWiseAttention(d_model)\n",
    "        \n",
    "        # Multi-stage output with residual connection\n",
    "        self.output_projection = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            activation_fn,\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Optional direct connection for residual learning\n",
    "        self.direct_connection = nn.Linear(1, 1)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize weights using Xavier uniform for better gradient flow\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, column_indices, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Expand to feature dimension if needed\n",
    "        x_input = x.unsqueeze(-1)  # [batch_size, num_features, 1]\n",
    "        \n",
    "        # Feature embedding\n",
    "        x_embed = self.feature_embedding(x_input)  # [batch_size, num_features, d_model]\n",
    "        \n",
    "        # Add column embeddings - provides context about which feature is being processed\n",
    "        col_embed = self.column_embedding(column_indices).unsqueeze(0)  # [1, num_features, d_model]\n",
    "        col_embed = col_embed.expand(batch_size, -1, -1)  # [batch_size, num_features, d_model]\n",
    "        x_embed = x_embed + col_embed\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x_embed = self.positional_encoding(x_embed)\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        if mask is not None:\n",
    "            x_encoded = self.transformer_encoder(x_embed, src_key_padding_mask=mask)\n",
    "        else:\n",
    "            x_encoded = self.transformer_encoder(x_embed)\n",
    "        \n",
    "        # Apply feature-wise attention\n",
    "        x_attended, attention_weights = self.feature_attention(x_encoded)\n",
    "        \n",
    "        # Generate output with residual connection\n",
    "        direct_out = self.direct_connection(x_input) if hasattr(self, 'direct_connection') else 0\n",
    "        output = self.output_projection(x_attended) + direct_out\n",
    "        \n",
    "        return output.squeeze(-1), attention_weights\n",
    "    \n",
    "    def impute(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Specialized method for imputation that handles missing value masks\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor with missing values (NaNs or specified value)\n",
    "            mask: Boolean mask where True indicates missing values\n",
    "            \n",
    "        Returns:\n",
    "            Imputed tensor\n",
    "        \"\"\"\n",
    "        # Create column indices\n",
    "        column_indices = torch.arange(self.num_features, device=x.device)\n",
    "        \n",
    "        # Forward pass with mask\n",
    "        imputed_values, _ = self.forward(x, column_indices, mask)\n",
    "        \n",
    "        # If mask is provided, only replace masked values\n",
    "        if mask is not None:\n",
    "            # Where mask is True, use imputed values, otherwise keep original\n",
    "            result = torch.where(mask, imputed_values, x)\n",
    "            return result\n",
    "        \n",
    "        return imputed_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_transformer(num_features, device, d_model=128, num_heads=8, num_layers=3):\n",
    "    model = EnhancedTransformerModel(\n",
    "        num_features=num_features,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        dropout=0.2,\n",
    "        use_layer_norm=True,\n",
    "        feedforward_dim=512,\n",
    "        activation='gelu'\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = train_tensor.shape[1]\n",
    "model = create_enhanced_transformer(num_features, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_indices = torch.arange(num_features).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1278, 39])\n",
      "Attention weights shape: torch.Size([1278, 39, 39])\n"
     ]
    }
   ],
   "source": [
    "output, attention_weights = model(train_tensor, column_indices)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
