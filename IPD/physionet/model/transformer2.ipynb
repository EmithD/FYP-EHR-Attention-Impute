{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_and_preprocess_data(file_path, scaler_type=\"robust\", test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Load and preprocess the data with multiple scaler options.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        scaler_type: Type of scaler to use ('standard', 'robust', or 'minmax')\n",
    "        test_size: Proportion of data to use for testing\n",
    "        random_state: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        train_tensor: Tensor of training data\n",
    "        test_tensor: Tensor of test data\n",
    "        scaler: Fitted scaler for future use\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, index_col=0)\n",
    "    data = df.to_numpy()\n",
    "    \n",
    "    # Split data into train and test sets\n",
    "    train_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Select scaler based on input\n",
    "    if scaler_type == \"standard\":\n",
    "        scaler = StandardScaler()\n",
    "    elif scaler_type == \"robust\":\n",
    "        scaler = RobustScaler()\n",
    "    elif scaler_type == \"minmax\":\n",
    "        scaler = MinMaxScaler()\n",
    "    else:\n",
    "        raise ValueError(\"Scaler type must be 'standard', 'robust', or 'minmax'\")\n",
    "    \n",
    "    # Scale the data\n",
    "    train_data = scaler.fit_transform(train_data)\n",
    "    test_data = scaler.transform(test_data)\n",
    "    \n",
    "    # Feature engineering: add interaction terms for selected features\n",
    "    # For demonstration, we'll create interactions between every 5th feature\n",
    "    # In practice, you would use domain knowledge to select meaningful interactions\n",
    "    \n",
    "    # Convert to tensors\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_tensor = torch.tensor(train_data, dtype=torch.float32).to(device)\n",
    "    test_tensor = torch.tensor(test_data, dtype=torch.float32).to(device)\n",
    "    \n",
    "    return train_tensor, test_tensor, scaler, device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedTransformerModel(nn.Module):\n",
    "    def __init__(self, num_features, d_model=128, num_heads=8, num_layers=4, \n",
    "                 dropout=0.1, dim_feedforward=512, activation=\"gelu\"):\n",
    "        super(EnhancedTransformerModel, self).__init__()\n",
    "        \n",
    "        # Initial embedding layer for each feature\n",
    "        self.embedding = nn.Linear(1, d_model)\n",
    "        \n",
    "        # Column embedding for feature position information\n",
    "        self.column_embedding = nn.Embedding(num_features, d_model)\n",
    "        \n",
    "        # Domain-specific initialization for embeddings (if available)\n",
    "        # Here we're using a simple uniform initialization as placeholder\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.column_embedding.weight)\n",
    "        \n",
    "        # Layer normalization before transformer\n",
    "        self.pre_norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Create stacked transformer layers with enhanced configuration\n",
    "        encoder_layers = []\n",
    "        for _ in range(num_layers):\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=num_heads,\n",
    "                dim_feedforward=dim_feedforward,\n",
    "                dropout=dropout,\n",
    "                activation=activation,\n",
    "                batch_first=True\n",
    "            )\n",
    "            encoder_layers.append(encoder_layer)\n",
    "        \n",
    "        self.transformer_encoder_layers = nn.ModuleList(encoder_layers)\n",
    "        \n",
    "        # Additional residual layers\n",
    "        self.residual_projections = nn.ModuleList([\n",
    "            nn.Linear(d_model, d_model) for _ in range(num_layers-1)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "        \n",
    "        # Initialize output layer with small weights\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight, gain=0.1)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "        \n",
    "    def forward(self, x, column_indices, mask=None, missing_mask=None):\n",
    "        # Unsqueeze to add feature dimension\n",
    "        x = x.unsqueeze(-1)\n",
    "        \n",
    "        # Embed each feature\n",
    "        x_embed = self.embedding(x)\n",
    "        \n",
    "        # Add column embeddings\n",
    "        column_embed = self.column_embedding(column_indices)\n",
    "        x_embed = x_embed + column_embed.unsqueeze(0)\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        x_embed = self.pre_norm(x_embed)\n",
    "        \n",
    "        # Pass through transformer layers with residual connections\n",
    "        x_encoded = x_embed\n",
    "        \n",
    "        # Store intermediate outputs for residual connections\n",
    "        intermediate_outputs = []\n",
    "        \n",
    "        for i, encoder_layer in enumerate(self.transformer_encoder_layers):\n",
    "            # Apply transformer layer\n",
    "            layer_output = encoder_layer(x_encoded, src_mask=mask)\n",
    "            \n",
    "            # For all but the first layer, add a residual connection from earlier layers\n",
    "            if i > 0:\n",
    "                # Apply residual projection\n",
    "                for j, prev_output in enumerate(intermediate_outputs):\n",
    "                    projected = self.residual_projections[j](prev_output)\n",
    "                    layer_output = layer_output + projected\n",
    "            \n",
    "            x_encoded = layer_output\n",
    "            intermediate_outputs.append(x_encoded)\n",
    "            \n",
    "        # Final output projection\n",
    "        output = self.output_layer(x_encoded)\n",
    "        \n",
    "        # Handle missing values with more sophisticated approach\n",
    "        if missing_mask is not None:\n",
    "            # For missing values, we can apply a separate processing\n",
    "            # Here we're just implementing a placeholder that could be enhanced\n",
    "            # with domain-specific logic\n",
    "            missing_indices = missing_mask == 1\n",
    "            # You could add special handling for missing values here\n",
    "        \n",
    "        return output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_mask(data, missing_fraction=0.2, strategy=\"random\"):\n",
    "    \"\"\"\n",
    "    Create a missing value mask with different strategies.\n",
    "    \n",
    "    Args:\n",
    "        data: Input tensor\n",
    "        missing_fraction: Fraction of values to mask\n",
    "        strategy: Strategy for masking ('random', 'structured', 'column')\n",
    "        \n",
    "    Returns:\n",
    "        mask: Binary mask where 1 indicates missing value\n",
    "    \"\"\"\n",
    "    if strategy == \"random\":\n",
    "        # Completely random masking (MCAR)\n",
    "        mask = torch.rand(data.shape).to(data.device) < missing_fraction\n",
    "        \n",
    "    elif strategy == \"structured\":\n",
    "        # Structured masking - values more likely to be missing if they're high (MNAR)\n",
    "        # This creates a pattern where higher values are more likely to be masked\n",
    "        probs = torch.sigmoid((data - data.mean()) / data.std() * 2)\n",
    "        mask = torch.rand(data.shape).to(data.device) < (probs * missing_fraction * 2)\n",
    "        \n",
    "        # Ensure we have approximately the right number of masked values\n",
    "        current_frac = mask.float().mean().item()\n",
    "        if current_frac > 0:\n",
    "            mask = mask * (missing_fraction / current_frac)\n",
    "            mask = torch.rand(data.shape).to(data.device) < mask\n",
    "        \n",
    "    elif strategy == \"column\":\n",
    "        # Column-based masking - some columns missing more than others\n",
    "        mask = torch.zeros_like(data, dtype=torch.bool)\n",
    "        num_cols = data.shape[1]\n",
    "        \n",
    "        # Randomly assign higher missing probability to some columns\n",
    "        col_probs = torch.ones(num_cols).to(data.device)\n",
    "        high_miss_cols = torch.randperm(num_cols)[:num_cols//3]\n",
    "        col_probs[high_miss_cols] = 3.0\n",
    "        \n",
    "        # Normalize to get the right overall fraction\n",
    "        col_probs = col_probs * (missing_fraction * num_cols / col_probs.sum())\n",
    "        \n",
    "        # Apply column-specific missing probabilities\n",
    "        for col in range(num_cols):\n",
    "            mask[:, col] = torch.rand(data.shape[0]).to(data.device) < col_probs[col]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Strategy must be 'random', 'structured', or 'column'\")\n",
    "    \n",
    "    return mask.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedImputableLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, beta=0.1):\n",
    "        super(EnhancedImputableLoss, self).__init__()\n",
    "        self.alpha = alpha  # Weight for MSE loss\n",
    "        self.beta = beta    # Weight for consistency loss\n",
    "        \n",
    "    def forward(self, predictions, ground_truth, mask):\n",
    "        \"\"\"\n",
    "        Compute a multi-component loss for imputation.\n",
    "        \n",
    "        Args:\n",
    "            predictions: Model predictions\n",
    "            ground_truth: True values\n",
    "            mask: Binary mask where 1 indicates missing/masked values\n",
    "            \n",
    "        Returns:\n",
    "            loss: Combined loss value\n",
    "        \"\"\"\n",
    "        # 1. MSE loss on masked values\n",
    "        mse_loss = nn.MSELoss(reduction='none')\n",
    "        element_loss = mse_loss(predictions, ground_truth)\n",
    "        masked_mse = (element_loss * mask).sum() / (mask.sum() + 1e-8)\n",
    "        \n",
    "        # 2. Consistency loss - predictions should be consistent with observed values\n",
    "        # For columns with both observed and missing values, the distributions should be similar\n",
    "        col_means_pred = predictions.mean(dim=0)\n",
    "        col_means_true = ground_truth.mean(dim=0)\n",
    "        consistency_loss = F.mse_loss(col_means_pred, col_means_true)\n",
    "        \n",
    "        # Combine losses\n",
    "        combined_loss = self.alpha * masked_mse + self.beta * consistency_loss\n",
    "        \n",
    "        return combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_tensor, criterion, column_indices, missing_fraction=0.2, masking_strategy=\"random\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_tensor: Test data\n",
    "        criterion: Loss function\n",
    "        column_indices: Feature indices\n",
    "        missing_fraction: Fraction of values to mask\n",
    "        masking_strategy: Strategy for creating missing values\n",
    "        \n",
    "    Returns:\n",
    "        loss: Test loss\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create missing mask\n",
    "    mask = create_missing_mask(test_tensor, missing_fraction, masking_strategy)\n",
    "    \n",
    "    # Apply mask to input data\n",
    "    input_with_mask = test_tensor.clone()\n",
    "    \n",
    "    # Use column means for missing values\n",
    "    col_means = torch.mean(test_tensor, dim=0, keepdim=True)\n",
    "    input_with_mask[mask == 1] = col_means.expand_as(test_tensor)[mask == 1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "        loss = criterion(predictions, test_tensor, mask)\n",
    "        \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_batches(model, train_tensor, criterion, optimizer, scheduler, \n",
    "                             column_indices, num_epochs=50, batch_size=32, \n",
    "                             missing_fraction=0.2, clip_value=1.0, \n",
    "                             masking_strategy=\"random\", \n",
    "                             validation_tensor=None, patience=5):\n",
    "    \"\"\"\n",
    "    Train the model with batches and advanced training features.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to train\n",
    "        train_tensor: Training data\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        column_indices: Feature indices\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        missing_fraction: Fraction of values to mask\n",
    "        clip_value: Gradient clipping value\n",
    "        masking_strategy: Strategy for creating missing values\n",
    "        validation_tensor: Optional validation data\n",
    "        patience: Early stopping patience\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        train_losses: List of training losses\n",
    "        val_losses: List of validation losses\n",
    "    \"\"\"\n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = TensorDataset(train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    device = train_tensor.device\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Use tqdm for progress bar\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get batch data\n",
    "            batch_data = batch[0]\n",
    "            \n",
    "            # Create missing mask for this batch\n",
    "            mask = create_missing_mask(batch_data, missing_fraction, masking_strategy)\n",
    "            \n",
    "            # Apply mask to input data\n",
    "            input_with_mask = batch_data.clone()\n",
    "            \n",
    "            # More sophisticated missing value handling\n",
    "            # Instead of replacing with zeros, use column means\n",
    "            col_means = torch.mean(batch_data, dim=0, keepdim=True)\n",
    "            input_with_mask[mask == 1] = col_means.expand_as(batch_data)[mask == 1]\n",
    "            \n",
    "            # Forward pass\n",
    "            predictions = model(input_with_mask, column_indices)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(predictions, batch_data, mask)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update progress bar\n",
    "            batch_loss = loss.item()\n",
    "            epoch_loss += batch_loss\n",
    "            progress_bar.set_postfix({\"batch_loss\": f\"{batch_loss:.4f}\"})\n",
    "        \n",
    "        # Calculate average epoch loss\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "        \n",
    "        # Validate if validation data is provided\n",
    "        if validation_tensor is not None:\n",
    "            val_loss = evaluate_model(model, validation_tensor, criterion, column_indices, missing_fraction)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Update learning rate based on validation loss\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                # Load best model\n",
    "                model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                break\n",
    "                \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_epoch_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_epoch_loss:.4f}\")\n",
    "            \n",
    "    return model, train_losses, val_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nrmse(predictions, ground_truth, mask):\n",
    "    \"\"\"\n",
    "    Compute Normalized Root Mean Square Error.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Model predictions\n",
    "        ground_truth: True values\n",
    "        mask: Binary mask where 1 indicates missing/masked values\n",
    "        \n",
    "    Returns:\n",
    "        nrmse: Normalized RMSE\n",
    "    \"\"\"\n",
    "    # Extract masked predictions and ground truth\n",
    "    masked_predictions = predictions[mask == 1]\n",
    "    masked_ground_truth = ground_truth[mask == 1]\n",
    "    \n",
    "    # Handle empty mask\n",
    "    if masked_predictions.numel() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute MSE and RMSE\n",
    "    mse = torch.mean((masked_predictions - masked_ground_truth) ** 2)\n",
    "    rmse = torch.sqrt(mse)\n",
    "    \n",
    "    # Compute data range for normalization\n",
    "    data_range = ground_truth.max() - ground_truth.min()\n",
    "    \n",
    "    # Normalize RMSE\n",
    "    nrmse = rmse / data_range\n",
    "    \n",
    "    return nrmse.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_enhanced_imputation_pipeline(file_path, scaler_type=\"robust\", num_epochs=50, \n",
    "                                      batch_size=32, d_model=128, num_heads=8, \n",
    "                                      num_layers=4, dim_feedforward=512, \n",
    "                                      learning_rate=1e-3, missing_fraction=0.2,\n",
    "                                      masking_strategy=\"random\"):\n",
    "    \"\"\"\n",
    "    Run the complete enhanced imputation pipeline.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        scaler_type: Type of scaler to use\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        d_model: Embedding dimension\n",
    "        num_heads: Number of attention heads\n",
    "        num_layers: Number of transformer layers\n",
    "        dim_feedforward: Dimension of feedforward network\n",
    "        learning_rate: Initial learning rate\n",
    "        missing_fraction: Fraction of values to mask\n",
    "        masking_strategy: Strategy for masking values\n",
    "        \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        train_losses: List of training losses\n",
    "        val_losses: List of validation losses\n",
    "        test_nrmse: NRMSE on test data\n",
    "        test_tensor: Test data tensor\n",
    "        column_indices: Feature indices\n",
    "    \"\"\"\n",
    "    # Load and preprocess data\n",
    "    train_tensor, test_tensor, scaler, device = load_and_preprocess_data(\n",
    "        file_path, scaler_type=scaler_type\n",
    "    )\n",
    "    \n",
    "    # Split train data into train and validation\n",
    "    val_size = int(0.1 * train_tensor.shape[0])\n",
    "    indices = torch.randperm(train_tensor.shape[0])\n",
    "    train_indices = indices[val_size:]\n",
    "    val_indices = indices[:val_size]\n",
    "    \n",
    "    train_data = train_tensor[train_indices]\n",
    "    val_data = train_tensor[val_indices]\n",
    "    \n",
    "    # Create column indices\n",
    "    num_features = train_data.shape[1]\n",
    "    column_indices = torch.arange(num_features).to(device)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = EnhancedTransformerModel(\n",
    "        num_features=num_features,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        num_layers=num_layers,\n",
    "        dim_feedforward=dim_feedforward\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize loss function\n",
    "    criterion = EnhancedImputableLoss(alpha=1.0, beta=0.1)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # Initialize learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "    \n",
    "    # Train model\n",
    "    model, train_losses, val_losses = train_model_with_batches(\n",
    "        model=model,\n",
    "        train_tensor=train_data,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        column_indices=column_indices,\n",
    "        num_epochs=num_epochs,\n",
    "        batch_size=batch_size,\n",
    "        missing_fraction=missing_fraction,\n",
    "        masking_strategy=masking_strategy,\n",
    "        validation_tensor=val_data,\n",
    "        patience=7\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    mask = create_missing_mask(test_tensor, missing_fraction, masking_strategy)\n",
    "    input_with_mask = test_tensor.clone()\n",
    "    \n",
    "    # Use column means for missing values\n",
    "    col_means = torch.mean(test_tensor, dim=0, keepdim=True)\n",
    "    input_with_mask[mask == 1] = col_means.expand_as(test_tensor)[mask == 1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_with_mask, column_indices)\n",
    "        \n",
    "    test_nrmse = compute_nrmse(predictions, test_tensor, mask)\n",
    "    print(f\"Test NRMSE: {test_nrmse:.4f}\")\n",
    "    \n",
    "    # Plot training and validation losses\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    if val_losses:\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scaler': scaler,\n",
    "        'num_features': num_features,\n",
    "        'config': {\n",
    "            'd_model': d_model,\n",
    "            'num_heads': num_heads,\n",
    "            'num_layers': num_layers,\n",
    "            'dim_feedforward': dim_feedforward,\n",
    "        }\n",
    "    }, \"enhanced_tabular_transformer.pth\")\n",
    "    \n",
    "    # Return test_tensor and column_indices as well\n",
    "    return model, train_losses, val_losses, test_nrmse, test_tensor, column_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_across_mechanisms(model, test_tensor, column_indices, missing_fractions):\n",
    "    \"\"\"\n",
    "    Evaluate model across different missing data mechanisms and fractions.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        test_tensor: Test data\n",
    "        column_indices: Feature indices\n",
    "        missing_fractions: List of missing data fractions to evaluate\n",
    "        \n",
    "    Returns:\n",
    "        results: Dictionary of results\n",
    "    \"\"\"\n",
    "    mechanisms = [\"random\", \"structured\", \"column\"]  # Corresponding to MCAR, MNAR, MAR\n",
    "    results = {mechanism: {} for mechanism in mechanisms}\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for mechanism in mechanisms:\n",
    "        for fraction in missing_fractions:\n",
    "            # Create missing mask\n",
    "            mask = create_missing_mask(test_tensor, fraction, mechanism)\n",
    "            \n",
    "            # Apply mask to input data\n",
    "            input_with_mask = test_tensor.clone()\n",
    "            \n",
    "            # Use column means for missing values\n",
    "            col_means = torch.mean(test_tensor, dim=0, keepdim=True)\n",
    "            input_with_mask[mask == 1] = col_means.expand_as(test_tensor)[mask == 1]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                predictions = model(input_with_mask, column_indices)\n",
    "                \n",
    "            nrmse = compute_nrmse(predictions, test_tensor, mask)\n",
    "            results[mechanism][fraction] = nrmse\n",
    "            \n",
    "            print(f\"{mechanism.upper()} NRMSE at {fraction * 100:.0f}% Missing: {nrmse:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_example(file_path):\n",
    "    # Run the enhanced pipeline and capture all return values including test_tensor and column_indices\n",
    "    model, train_losses, val_losses, test_nrmse, test_tensor, column_indices = run_enhanced_imputation_pipeline(\n",
    "        file_path=file_path,\n",
    "        scaler_type=\"robust\",\n",
    "        num_epochs=50,\n",
    "        batch_size=32,\n",
    "        d_model=128,\n",
    "        num_heads=8,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=512,\n",
    "        learning_rate=1e-3,\n",
    "        missing_fraction=0.2,\n",
    "        masking_strategy=\"random\"\n",
    "    )\n",
    "    \n",
    "    # Now we have test_tensor and column_indices available for the evaluation\n",
    "    missing_fractions = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    results = evaluate_across_mechanisms(model, test_tensor, column_indices, missing_fractions)\n",
    "    \n",
    "    # Print results\n",
    "    for mechanism, nrmse_values in results.items():\n",
    "        print(f\"\\n{mechanism.upper()} Results:\")\n",
    "        for frac, nrmse in nrmse_values.items():\n",
    "            print(f\"  Missing Fraction {frac * 100:.0f}%: NRMSE = {nrmse:.4f}\")\n",
    "    \n",
    "    return model, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wh1sper\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Epoch 1/50: 100%|██████████| 36/36 [00:02<00:00, 17.21it/s, batch_loss=0.7498]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 14.4135, Val Loss: 1.0541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 36/36 [00:02<00:00, 17.47it/s, batch_loss=5.1065] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 2.5376, Val Loss: 1.2830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 36/36 [00:02<00:00, 17.95it/s, batch_loss=19.6074]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 4.1467, Val Loss: 1.5181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 36/36 [00:01<00:00, 18.08it/s, batch_loss=0.7041] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 3.4012, Val Loss: 1.2784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 36/36 [00:02<00:00, 17.94it/s, batch_loss=1.2615]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 21.3196, Val Loss: 1.1844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 36/36 [00:02<00:00, 17.70it/s, batch_loss=1.0711] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 3.1713, Val Loss: 0.8324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 36/36 [00:02<00:00, 17.56it/s, batch_loss=3.5767] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 3.3324, Val Loss: 1.4788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 36/36 [00:02<00:00, 17.99it/s, batch_loss=1.1115]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 11.3079, Val Loss: 8.1062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 36/36 [00:02<00:00, 17.77it/s, batch_loss=0.7166]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 14.2671, Val Loss: 1.4528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 36/36 [00:02<00:00, 17.96it/s, batch_loss=0.5400]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 16.8474, Val Loss: 7.8352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 36/36 [00:02<00:00, 17.93it/s, batch_loss=0.9817]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 16.3153, Val Loss: 1.1013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 36/36 [00:02<00:00, 17.67it/s, batch_loss=10.7853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 2.3375, Val Loss: 0.8992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 36/36 [00:02<00:00, 17.83it/s, batch_loss=0.3334] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 13\n",
      "Test NRMSE: 0.0036\n",
      "RANDOM NRMSE at 10% Missing: 0.0023\n",
      "RANDOM NRMSE at 20% Missing: 0.0116\n",
      "RANDOM NRMSE at 30% Missing: 0.0043\n",
      "RANDOM NRMSE at 40% Missing: 0.0085\n",
      "RANDOM NRMSE at 50% Missing: 0.0085\n",
      "STRUCTURED NRMSE at 10% Missing: 0.0089\n",
      "STRUCTURED NRMSE at 20% Missing: 0.0199\n",
      "STRUCTURED NRMSE at 30% Missing: 0.0165\n",
      "STRUCTURED NRMSE at 40% Missing: 0.0169\n",
      "STRUCTURED NRMSE at 50% Missing: 0.0153\n",
      "COLUMN NRMSE at 10% Missing: 0.0085\n",
      "COLUMN NRMSE at 20% Missing: 0.0040\n",
      "COLUMN NRMSE at 30% Missing: 0.0095\n",
      "COLUMN NRMSE at 40% Missing: 0.0082\n",
      "COLUMN NRMSE at 50% Missing: 0.0150\n",
      "\n",
      "RANDOM Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0023\n",
      "  Missing Fraction 20%: NRMSE = 0.0116\n",
      "  Missing Fraction 30%: NRMSE = 0.0043\n",
      "  Missing Fraction 40%: NRMSE = 0.0085\n",
      "  Missing Fraction 50%: NRMSE = 0.0085\n",
      "\n",
      "STRUCTURED Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0089\n",
      "  Missing Fraction 20%: NRMSE = 0.0199\n",
      "  Missing Fraction 30%: NRMSE = 0.0165\n",
      "  Missing Fraction 40%: NRMSE = 0.0169\n",
      "  Missing Fraction 50%: NRMSE = 0.0153\n",
      "\n",
      "COLUMN Results:\n",
      "  Missing Fraction 10%: NRMSE = 0.0085\n",
      "  Missing Fraction 20%: NRMSE = 0.0040\n",
      "  Missing Fraction 30%: NRMSE = 0.0095\n",
      "  Missing Fraction 40%: NRMSE = 0.0082\n",
      "  Missing Fraction 50%: NRMSE = 0.0150\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Run the evaluation with the correct file path\n",
    "    run_evaluation_example(file_path='../data/physionet_wo_missing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
